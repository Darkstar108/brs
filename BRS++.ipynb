{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BRS++.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz67dvcyu6pA"
      },
      "source": [
        "# Imports\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "import os\r\n",
        "import os.path\r\n",
        "import shutil\r\n",
        "import hashlib\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.utils.checkpoint as cp\r\n",
        "import torch.utils.data as data\r\n",
        "from torch.utils.model_zoo import tqdm\r\n",
        "import tarfile\r\n",
        "import zipfile\r\n",
        "from torch import Tensor\r\n",
        "from torchvision import datasets, transforms\r\n",
        "from torchvision.datasets import SBDataset\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from PIL import Image\r\n",
        "from tqdm.auto import tqdm\r\n",
        "from collections import OrderedDict\r\n",
        "from typing import Any, List, Tuple, Optional, Callable, Dict, TypeVar, Iterable\r\n",
        "from vision import *\r\n",
        "from utils import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNjDh3pGt1hI"
      },
      "source": [
        "**Steps to do:**\r\n",
        "\r\n",
        "*   Preprocessing Pipeline\r\n",
        "  *   Part of Dataset init()\r\n",
        "     *   Ignore images with dimension lower than filter_size x filter_size\r\n",
        "     *   For images with multiple labels, pick one and let the others be part of the background\r\n",
        "  *   Part of Transform or Function\r\n",
        "     *   Take a filter_size x filter_size crop by resampling crops until the complete object instance is inside the crop\r\n",
        "     *   Upscale image to 480x480\r\n",
        "     *   Use k-mediods to generate random number of foreground & background clicks\r\n",
        "     *   Generate interaction maps\r\n",
        "*   Model\r\n",
        "  *   DenseNet-121 Encoder\r\n",
        "  *   Decoder from BRS paper\r\n",
        "  *   Add ASPP module between Encoder & Decoder\r\n",
        "  *   Add Semantic Supervision block to Encoder during pretraining\r\n",
        "  *   Add LIP for pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pQQOaT55kfY"
      },
      "source": [
        "# Function to convert PIL images to Tensors. We can pass this as a transform to the Dataset\r\n",
        "def PIL_to_tensor(img, target):\r\n",
        "  return transforms.ToTensor()(img), transforms.ToTensor()(target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FDsEkPyt4hK"
      },
      "source": [
        "def download_extract(url: str, root: str, filename: str, md5: str) -> None:\r\n",
        "    download_url(url, root, filename, md5)\r\n",
        "    with tarfile.open(os.path.join(root, filename), \"r\") as tar:\r\n",
        "        tar.extractall(path=root)\r\n",
        "\r\n",
        "# Remember to upload vision.py and utils.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpdKdu90t7XL"
      },
      "source": [
        "class CustomSBDataset(VisionDataset):\r\n",
        "    \"\"\"`Semantic Boundaries Dataset <http://home.bharathh.info/pubs/codes/SBD/download.html>`_\r\n",
        "\r\n",
        "    Args:\r\n",
        "        root (string): Root directory of the Semantic Boundaries Dataset\r\n",
        "        image_set (string, optional): Select the image_set to use, ``train``, ``val`` or ``train_noval``.\r\n",
        "            Image set ``train_noval`` excludes VOC 2012 val images.\r\n",
        "        mode (string, optional): Select target type. Possible values 'boundaries' or 'segmentation'.\r\n",
        "            In case of 'boundaries', the target is an array of shape `[num_classes, H, W]`,\r\n",
        "            where `num_classes=20`.\r\n",
        "        download (bool, optional): If true, downloads the dataset from the internet and\r\n",
        "            puts it in root directory. If dataset is already downloaded, it is not\r\n",
        "            downloaded again.\r\n",
        "        transforms (callable, optional): A function/transform that takes input sample and its target as entry\r\n",
        "            and returns a transformed version. Input sample is PIL image and target is a numpy array\r\n",
        "            if `mode='boundaries'` or PIL image if `mode='segmentation'`.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    url = \"http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/semantic_contours/benchmark.tgz\"\r\n",
        "    md5 = \"82b4d87ceb2ed10f6038a1cba92111cb\"\r\n",
        "    filename = \"benchmark.tgz\"\r\n",
        "\r\n",
        "    voc_train_url = \"http://home.bharathh.info/pubs/codes/SBD/train_noval.txt\"\r\n",
        "    voc_split_filename = \"train_noval.txt\"\r\n",
        "    voc_split_md5 = \"79bff800c5f0b1ec6b21080a3c066722\"\r\n",
        "\r\n",
        "    def __init__(\r\n",
        "            self,\r\n",
        "            root: str,\r\n",
        "            image_set: str = \"train\",\r\n",
        "            mode: str = \"boundaries\",\r\n",
        "            download: bool = False,\r\n",
        "            transforms: Optional[Callable] = None,\r\n",
        "    ) -> None:\r\n",
        "\r\n",
        "        try:\r\n",
        "            from scipy.io import loadmat\r\n",
        "            self._loadmat = loadmat\r\n",
        "        except ImportError:\r\n",
        "            raise RuntimeError(\"Scipy is not found. This dataset needs to have scipy installed: \"\r\n",
        "                               \"pip install scipy\")\r\n",
        "\r\n",
        "        super(CustomSBDataset, self).__init__(root, transforms)\r\n",
        "        self.image_set = verify_str_arg(image_set, \"image_set\",\r\n",
        "                                        (\"train\", \"val\", \"train_noval\"))\r\n",
        "        self.mode = verify_str_arg(mode, \"mode\", (\"segmentation\", \"boundaries\"))\r\n",
        "        self.num_classes = 20\r\n",
        "\r\n",
        "        sbd_root = self.root\r\n",
        "        image_dir = os.path.join(sbd_root, 'img')\r\n",
        "        mask_dir = os.path.join(sbd_root, 'inst')\r\n",
        "\r\n",
        "        if download:\r\n",
        "            self.download_dataset(sbd_root)\r\n",
        "\r\n",
        "        if not os.path.isdir(sbd_root):\r\n",
        "            raise RuntimeError('Dataset not found or corrupted.' +\r\n",
        "                               ' You can use download=True to download it')\r\n",
        "            \r\n",
        "        self.load_filenames(sbd_root, image_dir, mask_dir, image_set)\r\n",
        "\r\n",
        "        self._get_target = self._get_segmentation_target \\\r\n",
        "            if self.mode == \"segmentation\" else self._get_boundaries_target\r\n",
        "\r\n",
        "    def _get_segmentation_target(self, filepath: str) -> Image.Image:\r\n",
        "        mat = self._loadmat(filepath)\r\n",
        "        return Image.fromarray(mat['GTinst'][0]['Segmentation'][0])\r\n",
        "\r\n",
        "    def _get_boundaries_target(self, filepath: str) -> np.ndarray:\r\n",
        "        mat = self._loadmat(filepath)\r\n",
        "        return np.concatenate([np.expand_dims(mat['GTinst'][0]['Boundaries'][0][i][0].toarray(), axis=0)\r\n",
        "                               for i in range(self.num_classes)], axis=0)\r\n",
        "\r\n",
        "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\r\n",
        "        img = Image.open(self.images[index]).convert('RGB')\r\n",
        "        target = self._get_target(self.masks[index])\r\n",
        "\r\n",
        "        if self.transforms is not None:\r\n",
        "            img, target = self.transforms(img, target)\r\n",
        "\r\n",
        "        return img, target\r\n",
        "\r\n",
        "    def __len__(self) -> int:\r\n",
        "        return len(self.images)\r\n",
        "\r\n",
        "    def extra_repr(self) -> str:\r\n",
        "        lines = [\"Image set: {image_set}\", \"Mode: {mode}\"]\r\n",
        "        return '\\n'.join(lines).format(**self.__dict__)\r\n",
        "\r\n",
        "    def download_dataset(self, sbd_root) -> None:\r\n",
        "        download_extract(self.url, self.root, self.filename, self.md5)\r\n",
        "        extracted_ds_root = os.path.join(self.root, \"benchmark_RELEASE\", \"dataset\")\r\n",
        "\r\n",
        "        for f in [\"cls\", \"img\", \"inst\", \"train.txt\", \"val.txt\"]:\r\n",
        "            old_path = os.path.join(extracted_ds_root, f)\r\n",
        "            shutil.move(old_path, sbd_root)\r\n",
        "        download_url(self.voc_train_url, sbd_root, self.voc_split_filename,\r\n",
        "                      self.voc_split_md5)\r\n",
        "        \r\n",
        "    def load_filenames(self, sbd_root, image_dir, mask_dir, image_set) -> None:\r\n",
        "        split_f = os.path.join(sbd_root, image_set.rstrip('\\n') + '.txt')\r\n",
        "\r\n",
        "        with open(os.path.join(split_f), \"r\") as fh:\r\n",
        "            file_names = [x.strip() for x in fh.readlines()]\r\n",
        "\r\n",
        "        self.images = [os.path.join(image_dir, x + \".jpg\") for x in file_names]\r\n",
        "        self.masks = [os.path.join(mask_dir, x + \".mat\") for x in file_names]\r\n",
        "        assert (len(self.images) == len(self.masks))\r\n",
        "        \r\n",
        "    # Remove images with both height and width lower than img_filter_size\r\n",
        "    def filter_images(self, img_filter_size):\r\n",
        "        img_to_remove = set()\r\n",
        "        for i in range(len(self.images)):\r\n",
        "            img, target = self.__getitem__(i)\r\n",
        "            width, height = img.size\r\n",
        "            if width < img_filter_size or height < img_filter_size:\r\n",
        "                img_to_remove.add(i)\r\n",
        "\r\n",
        "        self.images = [img for index, img in enumerate(self.images) if index not in img_to_remove]\r\n",
        "        self.masks = [mask for index, mask in enumerate(self.masks) if index not in img_to_remove]\r\n",
        "\r\n",
        "    # For targets that have multiple object instances, pick one as foreground and let the others be part of the background\r\n",
        "    def ignore_multiple_object_instances(self):\r\n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGBgvltX5t8_"
      },
      "source": [
        "# Download the training Dataset\r\n",
        "sbd = CustomSBDataset(root=\".\", image_set=\"train\", mode=\"segmentation\", download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W19rDu1Q5-o1"
      },
      "source": [
        "# Download the validation Dataset\r\n",
        "sbd_val = CustomSBDataset(root=\".\", image_set=\"val\", mode=\"segmentation\", download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXRfmf7e6E22"
      },
      "source": [
        "# Code used to explore Image dimensions\r\n",
        "max_h = 0\r\n",
        "max_w = 0\r\n",
        "min_h = 500\r\n",
        "min_w = 500\r\n",
        "c_280 = 0\r\n",
        "c_300 = 0\r\n",
        "c_310 = 0\r\n",
        "c_320 = 0\r\n",
        "c_330 = 0\r\n",
        "c_340 = 0\r\n",
        "c_350 = 0\r\n",
        "c_360 = 0\r\n",
        "\r\n",
        "for i in range(sbd.__len__()):\r\n",
        "  img, target = sbd.__getitem__(i)\r\n",
        "\r\n",
        "  if img.shape[1] > max_h:\r\n",
        "    max_h = img.shape[1]\r\n",
        "  if img.shape[1] < min_h:\r\n",
        "    min_h = img.shape[1]\r\n",
        "  if img.shape[2] > max_w:\r\n",
        "    max_w = img.shape[2]\r\n",
        "  if img.shape[2] < min_w:\r\n",
        "    min_w = img.shape[2]\r\n",
        "\r\n",
        "  if img.shape[1] > 280 and img.shape[2] > 280:\r\n",
        "    c_280 += 1\r\n",
        "  if img.shape[1] > 300 and img.shape[2] > 300:\r\n",
        "    c_300 += 1\r\n",
        "  if img.shape[1] > 310 and img.shape[2] > 310:\r\n",
        "    c_310 += 1\r\n",
        "  if img.shape[1] > 320 and img.shape[2] > 320:\r\n",
        "    c_320 += 1\r\n",
        "  if img.shape[1] > 330 and img.shape[2] > 330:\r\n",
        "    c_330 += 1\r\n",
        "  if img.shape[1] > 340 and img.shape[2] > 340:\r\n",
        "    c_340 += 1\r\n",
        "  if img.shape[1] > 350 and img.shape[2] > 350:\r\n",
        "    c_350 += 1\r\n",
        "  if img.shape[1] > 360 and img.shape[2] > 360:\r\n",
        "    c_360 += 1\r\n",
        "\r\n",
        "print(\"Max Height & Width:\", max_h, max_w)\r\n",
        "print(\"Min Height & Width:\", min_h, min_w)\r\n",
        "print(\"Images over 280x280:\", c_280, (c_280*100)/sbd.__len__())\r\n",
        "print(\"Images over 300x300:\", c_300, (c_300*100)/sbd.__len__())\r\n",
        "print(\"Images over 310x310:\", c_310, (c_310*100)/sbd.__len__())\r\n",
        "print(\"Images over 320x320:\", c_320, (c_320*100)/sbd.__len__())\r\n",
        "print(\"Images over 330x330:\", c_330, (c_330*100)/sbd.__len__())\r\n",
        "print(\"Images over 340x340:\", c_340, (c_340*100)/sbd.__len__())\r\n",
        "print(\"Images over 350x350:\", c_350, (c_350*100)/sbd.__len__())\r\n",
        "print(\"Images over 360x360:\", c_360, (c_360*100)/sbd.__len__())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_qE74Q06PNt"
      },
      "source": [
        "# Code to display image and segmentation mask. Set images to be num of image, segmentation masks to display\r\n",
        "images = 10\r\n",
        "fig,axes = plt.subplots(nrows = images, ncols = 2, figsize=(50,50))\r\n",
        "\r\n",
        "for i in range(images):\r\n",
        "    img, target = sbd.__getitem__(i)\r\n",
        "    axes[i,0].imshow(img)\r\n",
        "    axes[i,1].imshow(target)\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dKzk186FYEV"
      },
      "source": [
        "\"\"\"\r\n",
        "DenseNet Encoder Network\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "class _DenseLayer(nn.Module):\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        num_input_features: int,\r\n",
        "        growth_rate: int,\r\n",
        "        bn_size: int,\r\n",
        "        drop_rate: float,\r\n",
        "        memory_efficient: bool = False\r\n",
        "    ) -> None:\r\n",
        "        super(_DenseLayer, self).__init__()\r\n",
        "        self.norm1: nn.BatchNorm2d\r\n",
        "        self.add_module('norm1', nn.BatchNorm2d(num_input_features))\r\n",
        "        self.relu1: nn.ReLU\r\n",
        "        self.add_module('relu1', nn.ReLU(inplace=True))\r\n",
        "        self.conv1: nn.Conv2d\r\n",
        "        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\r\n",
        "                                           growth_rate, kernel_size=1, stride=1,\r\n",
        "                                           bias=False))\r\n",
        "        self.norm2: nn.BatchNorm2d\r\n",
        "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate))\r\n",
        "        self.relu2: nn.ReLU\r\n",
        "        self.add_module('relu2', nn.ReLU(inplace=True))\r\n",
        "        self.conv2: nn.Conv2d\r\n",
        "        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\r\n",
        "                                           kernel_size=3, stride=1, padding=1,\r\n",
        "                                           bias=False))\r\n",
        "        self.drop_rate = float(drop_rate)\r\n",
        "        self.memory_efficient = memory_efficient\r\n",
        "\r\n",
        "    def bn_function(self, inputs: List[Tensor]) -> Tensor:\r\n",
        "        concated_features = torch.cat(inputs, 1)\r\n",
        "        bottleneck_output = self.conv1(self.relu1(self.norm1(concated_features)))  # noqa: T484\r\n",
        "        return bottleneck_output\r\n",
        "\r\n",
        "    # todo: rewrite when torchscript supports any\r\n",
        "    def any_requires_grad(self, input: List[Tensor]) -> bool:\r\n",
        "        for tensor in input:\r\n",
        "            if tensor.requires_grad:\r\n",
        "                return True\r\n",
        "        return False\r\n",
        "\r\n",
        "    @torch.jit.unused  # noqa: T484\r\n",
        "    def call_checkpoint_bottleneck(self, input: List[Tensor]) -> Tensor:\r\n",
        "        def closure(*inputs):\r\n",
        "            return self.bn_function(inputs)\r\n",
        "\r\n",
        "        return cp.checkpoint(closure, *input)\r\n",
        "\r\n",
        "    @torch.jit._overload_method  # noqa: F811\r\n",
        "    def forward(self, input: List[Tensor]) -> Tensor:\r\n",
        "        pass\r\n",
        "\r\n",
        "    @torch.jit._overload_method  # noqa: F811\r\n",
        "    def forward(self, input: Tensor) -> Tensor:\r\n",
        "        pass\r\n",
        "\r\n",
        "    # torchscript does not yet support *args, so we overload method\r\n",
        "    # allowing it to take either a List[Tensor] or single Tensor\r\n",
        "    def forward(self, input: Tensor) -> Tensor:  # noqa: F811\r\n",
        "        if isinstance(input, Tensor):\r\n",
        "            prev_features = [input]\r\n",
        "        else:\r\n",
        "            prev_features = input\r\n",
        "\r\n",
        "        if self.memory_efficient and self.any_requires_grad(prev_features):\r\n",
        "            if torch.jit.is_scripting():\r\n",
        "                raise Exception(\"Memory Efficient not supported in JIT\")\r\n",
        "\r\n",
        "            bottleneck_output = self.call_checkpoint_bottleneck(prev_features)\r\n",
        "        else:\r\n",
        "            bottleneck_output = self.bn_function(prev_features)\r\n",
        "\r\n",
        "        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\r\n",
        "        if self.drop_rate > 0:\r\n",
        "            new_features = F.dropout(new_features, p=self.drop_rate,\r\n",
        "                                     training=self.training)\r\n",
        "        return new_features\r\n",
        "\r\n",
        "\r\n",
        "class _DenseBlock(nn.ModuleDict):\r\n",
        "    _version = 2\r\n",
        "\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        num_layers: int,\r\n",
        "        num_input_features: int,\r\n",
        "        bn_size: int,\r\n",
        "        growth_rate: int,\r\n",
        "        drop_rate: float,\r\n",
        "        memory_efficient: bool = False\r\n",
        "    ) -> None:\r\n",
        "        super(_DenseBlock, self).__init__()\r\n",
        "        for i in range(num_layers):\r\n",
        "            layer = _DenseLayer(\r\n",
        "                num_input_features + i * growth_rate,\r\n",
        "                growth_rate=growth_rate,\r\n",
        "                bn_size=bn_size,\r\n",
        "                drop_rate=drop_rate,\r\n",
        "                memory_efficient=memory_efficient,\r\n",
        "            )\r\n",
        "            self.add_module('denselayer%d' % (i + 1), layer)\r\n",
        "\r\n",
        "    def forward(self, init_features: Tensor) -> Tensor:\r\n",
        "        features = [init_features]\r\n",
        "        for name, layer in self.items():\r\n",
        "            new_features = layer(features)\r\n",
        "            features.append(new_features)\r\n",
        "        return torch.cat(features, 1)\r\n",
        "\r\n",
        "\r\n",
        "class _Transition(nn.Sequential):\r\n",
        "    def __init__(self, num_input_features: int, num_output_features: int) -> None:\r\n",
        "        super(_Transition, self).__init__()\r\n",
        "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\r\n",
        "        self.add_module('relu', nn.ReLU(inplace=True))\r\n",
        "        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\r\n",
        "                                          kernel_size=1, stride=1, bias=False))\r\n",
        "        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\r\n",
        "\r\n",
        "\r\n",
        "class DenseNet(nn.Module):\r\n",
        "    \"\"\"Densenet-BC model class, based on\r\n",
        "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_.\r\n",
        "    Args:\r\n",
        "        growth_rate (int) - how many filters to add each layer (`k` in paper)\r\n",
        "        block_config (list of 4 ints) - how many layers in each pooling block\r\n",
        "        num_init_features (int) - the number of filters to learn in the first convolution layer\r\n",
        "        bn_size (int) - multiplicative factor for number of bottle neck layers\r\n",
        "          (i.e. bn_size * k features in the bottleneck layer)\r\n",
        "        drop_rate (float) - dropout rate after each dense layer\r\n",
        "        num_classes (int) - number of classification classes\r\n",
        "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\r\n",
        "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        growth_rate: int = 32,\r\n",
        "        block_config: Tuple[int, int, int, int] = (6, 12, 24, 16),\r\n",
        "        num_init_features: int = 64,\r\n",
        "        bn_size: int = 4,\r\n",
        "        drop_rate: float = 0,\r\n",
        "        num_classes: int = 1000,\r\n",
        "        memory_efficient: bool = False\r\n",
        "    ) -> None:\r\n",
        "\r\n",
        "        super(DenseNet, self).__init__()\r\n",
        "\r\n",
        "        # First convolution\r\n",
        "        self.features = nn.Sequential(OrderedDict([\r\n",
        "            ('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2,\r\n",
        "                                padding=3, bias=False)),\r\n",
        "            ('norm0', nn.BatchNorm2d(num_init_features)),\r\n",
        "            ('relu0', nn.ReLU(inplace=True)),\r\n",
        "            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\r\n",
        "        ]))\r\n",
        "\r\n",
        "        # Each denseblock\r\n",
        "        num_features = num_init_features\r\n",
        "        for i, num_layers in enumerate(block_config):\r\n",
        "            block = _DenseBlock(\r\n",
        "                num_layers=num_layers,\r\n",
        "                num_input_features=num_features,\r\n",
        "                bn_size=bn_size,\r\n",
        "                growth_rate=growth_rate,\r\n",
        "                drop_rate=drop_rate,\r\n",
        "                memory_efficient=memory_efficient\r\n",
        "            )\r\n",
        "            self.features.add_module('denseblock%d' % (i + 1), block)\r\n",
        "            num_features = num_features + num_layers * growth_rate\r\n",
        "            if i != len(block_config) - 1:\r\n",
        "                trans = _Transition(num_input_features=num_features,\r\n",
        "                                    num_output_features=num_features // 2)\r\n",
        "                self.features.add_module('transition%d' % (i + 1), trans)\r\n",
        "                num_features = num_features // 2\r\n",
        "\r\n",
        "        # Final batch norm\r\n",
        "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\r\n",
        "\r\n",
        "        # Linear layer\r\n",
        "        self.classifier = nn.Linear(num_features, num_classes)\r\n",
        "\r\n",
        "        # Official init from torch repo.\r\n",
        "        for m in self.modules():\r\n",
        "            if isinstance(m, nn.Conv2d):\r\n",
        "                nn.init.kaiming_normal_(m.weight)\r\n",
        "            elif isinstance(m, nn.BatchNorm2d):\r\n",
        "                nn.init.constant_(m.weight, 1)\r\n",
        "                nn.init.constant_(m.bias, 0)\r\n",
        "            elif isinstance(m, nn.Linear):\r\n",
        "                nn.init.constant_(m.bias, 0)\r\n",
        "\r\n",
        "    def forward(self, x: Tensor) -> Tensor:\r\n",
        "        features = self.features(x)\r\n",
        "        out = F.relu(features, inplace=True)\r\n",
        "        out = F.adaptive_avg_pool2d(out, (1, 1))\r\n",
        "        out = torch.flatten(out, 1)\r\n",
        "        out = self.classifier(out)\r\n",
        "        return out\r\n",
        "\r\n",
        "\r\n",
        "def _load_state_dict(model: nn.Module, model_url: str, progress: bool) -> None:\r\n",
        "    # '.'s are no longer allowed in module names, but previous _DenseLayer\r\n",
        "    # has keys 'norm.1', 'relu.1', 'conv.1', 'norm.2', 'relu.2', 'conv.2'.\r\n",
        "    # They are also in the checkpoints in model_urls. This pattern is used\r\n",
        "    # to find such keys.\r\n",
        "    pattern = re.compile(\r\n",
        "        r'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$')\r\n",
        "\r\n",
        "    state_dict = load_state_dict_from_url(model_url, progress=progress)\r\n",
        "    for key in list(state_dict.keys()):\r\n",
        "        res = pattern.match(key)\r\n",
        "        if res:\r\n",
        "            new_key = res.group(1) + res.group(2)\r\n",
        "            state_dict[new_key] = state_dict[key]\r\n",
        "            del state_dict[key]\r\n",
        "    model.load_state_dict(state_dict)\r\n",
        "\r\n",
        "\r\n",
        "def _densenet(\r\n",
        "    arch: str,\r\n",
        "    growth_rate: int,\r\n",
        "    block_config: Tuple[int, int, int, int],\r\n",
        "    num_init_features: int,\r\n",
        "    pretrained: bool,\r\n",
        "    progress: bool,\r\n",
        "    **kwargs: Any\r\n",
        ") -> DenseNet:\r\n",
        "    model = DenseNet(growth_rate, block_config, num_init_features, **kwargs)\r\n",
        "    if pretrained:\r\n",
        "        _load_state_dict(model, model_urls[arch], progress)\r\n",
        "    return model\r\n",
        "\r\n",
        "\r\n",
        "def densenet121(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> DenseNet:\r\n",
        "    \"\"\"Densenet-121 model from\r\n",
        "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_.\r\n",
        "    Args:\r\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\r\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\r\n",
        "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\r\n",
        "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_.\r\n",
        "    \"\"\"\r\n",
        "    return _densenet('densenet121', 32, (6, 12, 24, 16), 64, pretrained, progress,\r\n",
        "                     **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}