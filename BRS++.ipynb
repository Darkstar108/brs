{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BRS++.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "56233bc668204ee88bd60d66467862f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ffb44b1ceccd4bf1be0d8af19c7c23ec",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fab5b2f6ddd247bb8f972dc6c92cc0b3",
              "IPY_MODEL_7f0759b66de0458691e16a27964c783e"
            ]
          }
        },
        "ffb44b1ceccd4bf1be0d8af19c7c23ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fab5b2f6ddd247bb8f972dc6c92cc0b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_92a789bd1f5d43efa0508d8d3a197ce5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2718725d50c942629f0c9c7e9a19e089"
          }
        },
        "7f0759b66de0458691e16a27964c783e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_294c0b4c95c04dc8a2b0b65049c7ad9e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1419542528/? [08:50&lt;00:00, 2108915.39it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_347cd3a29409469d8a5e9a2c48195482"
          }
        },
        "92a789bd1f5d43efa0508d8d3a197ce5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2718725d50c942629f0c9c7e9a19e089": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "294c0b4c95c04dc8a2b0b65049c7ad9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "347cd3a29409469d8a5e9a2c48195482": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "db1c28075da3434582305076998c135e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d4d2c5e54ff54661b0fa95b737222966",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6946fa8419224c969464099429a12d2c",
              "IPY_MODEL_0e708e48e971438387884c21cd3ad8fc"
            ]
          }
        },
        "d4d2c5e54ff54661b0fa95b737222966": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6946fa8419224c969464099429a12d2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4a68094db68d4e14b3532e0f819d2ddb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d5bdbcfa9af64c44a24fd43b49c38836"
          }
        },
        "0e708e48e971438387884c21cd3ad8fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_06e3a293a2f94bb49be2d4adb6f96af6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 73728/? [00:00&lt;00:00, 173712.03it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5248fb0c55804208808e51cbfeacf57a"
          }
        },
        "4a68094db68d4e14b3532e0f819d2ddb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d5bdbcfa9af64c44a24fd43b49c38836": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "06e3a293a2f94bb49be2d4adb6f96af6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5248fb0c55804208808e51cbfeacf57a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJM_TA69jfX9",
        "outputId": "baaaa76c-2ad1-4986-e881-800f92d66656"
      },
      "source": [
        "%pip install scikit-learn-extra"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn-extra\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/1e/79cd1a2b60a6cd6a7a9c4719f947e264ae391644adce3ddecf73afd5233d/scikit_learn_extra-0.2.0-cp37-cp37m-manylinux2010_x86_64.whl (1.7MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.7MB 5.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-extra) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-extra) (1.4.1)\n",
            "Collecting scikit-learn>=0.23.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/eb/a48f25c967526b66d5f1fa7a984594f0bf0a5afafa94a8c4dbc317744620/scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22.3MB 1.1MB/s \n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.0.1)\n",
            "Installing collected packages: threadpoolctl, scikit-learn, scikit-learn-extra\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.24.2 scikit-learn-extra-0.2.0 threadpoolctl-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz67dvcyu6pA"
      },
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from scipy import ndimage\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import os.path\n",
        "import random\n",
        "import shutil\n",
        "import hashlib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint as cp\n",
        "import torch.utils.data as data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.model_zoo import tqdm\n",
        "import tarfile\n",
        "import zipfile\n",
        "import torchvision\n",
        "from torch import Tensor\n",
        "from torchvision import datasets\n",
        "from torchvision.datasets import SBDataset, CIFAR10\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets.vision import VisionDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "from collections import OrderedDict\n",
        "from typing import Any, List, Tuple, Optional, Callable, Dict, TypeVar, Iterable\n",
        "import torch.optim as optim\n",
        "from vision import *\n",
        "from utils import *\n",
        "\n",
        "random.seed(0)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pQQOaT55kfY"
      },
      "source": [
        "# Function to convert PIL images to Tensors. We can pass this as a transform to the Dataset\n",
        "def PIL_to_tensor(img, target):\n",
        "  return transforms.ToTensor()(img), transforms.ToTensor()(target)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FDsEkPyt4hK"
      },
      "source": [
        "def download_extract(url: str, root: str, filename: str, md5: str) -> None:\n",
        "    download_url(url, root, filename, md5)\n",
        "    with tarfile.open(os.path.join(root, filename), \"r\") as tar:\n",
        "        tar.extractall(path=root)\n",
        "\n",
        "# Remember to upload vision.py and utils.py"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpdKdu90t7XL"
      },
      "source": [
        "class CustomSBDataset(VisionDataset):\n",
        "    \"\"\"`Semantic Boundaries Dataset <http://home.bharathh.info/pubs/codes/SBD/download.html>`_\n",
        "\n",
        "    Args:\n",
        "        root (string): Root directory of the Semantic Boundaries Dataset\n",
        "        image_set (string, optional): Select the image_set to use, ``train``, ``val`` or ``train_noval``.\n",
        "            Image set ``train_noval`` excludes VOC 2012 val images.\n",
        "        mode (string, optional): Select target type. Possible values 'boundaries' or 'segmentation'.\n",
        "            In case of 'boundaries', the target is an array of shape `[num_classes, H, W]`,\n",
        "            where `num_classes=20`.\n",
        "        download (bool, optional): If true, downloads the dataset from the internet and\n",
        "            puts it in root directory. If dataset is already downloaded, it is not\n",
        "            downloaded again.\n",
        "        transforms (callable, optional): A function/transform that takes input sample and its target as entry\n",
        "            and returns a transformed version. Input sample is PIL image and target is a numpy array\n",
        "            if `mode='boundaries'` or PIL image if `mode='segmentation'`.\n",
        "    \"\"\"\n",
        "\n",
        "    url = \"http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/semantic_contours/benchmark.tgz\"\n",
        "    md5 = \"82b4d87ceb2ed10f6038a1cba92111cb\"\n",
        "    filename = \"benchmark.tgz\"\n",
        "\n",
        "    voc_train_url = \"http://home.bharathh.info/pubs/codes/SBD/train_noval.txt\"\n",
        "    voc_split_filename = \"train_noval.txt\"\n",
        "    voc_split_md5 = \"79bff800c5f0b1ec6b21080a3c066722\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            root: str,\n",
        "            image_set: str = \"train\",\n",
        "            mode: str = \"segmentation\",\n",
        "            download: bool = False,\n",
        "            img_filter_size = 200,\n",
        "            transforms: Optional[Callable] = None,\n",
        "    ) -> None:\n",
        "\n",
        "        try:\n",
        "            from scipy.io import loadmat\n",
        "            self._loadmat = loadmat\n",
        "        except ImportError:\n",
        "            raise RuntimeError(\"Scipy is not found. This dataset needs to have scipy installed: \"\n",
        "                               \"pip install scipy\")\n",
        "\n",
        "        super(CustomSBDataset, self).__init__(root, transforms)\n",
        "        self.image_set = verify_str_arg(image_set, \"image_set\",\n",
        "                                        (\"train\", \"val\", \"train_noval\"))\n",
        "        self.mode = verify_str_arg(mode, \"mode\", (\"segmentation\", \"boundaries\"))\n",
        "        self.num_classes = 20\n",
        "        self.img_filter_size = img_filter_size\n",
        "\n",
        "        sbd_root = self.root\n",
        "        image_dir = os.path.join(sbd_root, 'img')\n",
        "        mask_dir = os.path.join(sbd_root, 'inst')\n",
        "\n",
        "        if download:\n",
        "            self.download_dataset(sbd_root)\n",
        "\n",
        "        if not os.path.isdir(sbd_root):\n",
        "            raise RuntimeError('Dataset not found or corrupted.' +\n",
        "                               ' You can use download=True to download it')\n",
        "            \n",
        "        self.load_filenames(sbd_root, image_dir, mask_dir, image_set)\n",
        "\n",
        "        self._get_target = self._get_segmentation_target \\\n",
        "            if self.mode == \"segmentation\" else self._get_boundaries_target\n",
        "\n",
        "        #self.filter_images()\n",
        "        self.remove_images_without_mask()\n",
        "\n",
        "    def _get_segmentation_target(self, filepath: str) -> Image.Image:\n",
        "        mat = self._loadmat(filepath)\n",
        "        return Image.fromarray(mat['GTinst'][0]['Segmentation'][0])\n",
        "\n",
        "    def _get_boundaries_target(self, filepath: str) -> np.ndarray:\n",
        "        mat = self._loadmat(filepath)\n",
        "        return np.concatenate([np.expand_dims(mat['GTinst'][0]['Boundaries'][0][i][0].toarray(), axis=0)\n",
        "                               for i in range(self.num_classes)], axis=0)\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
        "        img = Image.open(self.images[index]).convert('RGB')\n",
        "        target = self._get_target(self.masks[index])\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        img = transforms.ToTensor()(img)\n",
        "        target = (transforms.ToTensor()(target)*255)\n",
        "        target = self.naive_ignore_multiple_object_instances(target)\n",
        "        img, target = self.center_crop(img, target)\n",
        "        img = transforms.Resize((480, 480))(img)\n",
        "        target = transforms.Resize((480, 480))(target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.images)\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        lines = [\"Image set: {image_set}\", \"Mode: {mode}\"]\n",
        "        return '\\n'.join(lines).format(**self.__dict__)\n",
        "\n",
        "    def download_dataset(self, sbd_root) -> None:\n",
        "        download_extract(self.url, self.root, self.filename, self.md5)\n",
        "        extracted_ds_root = os.path.join(self.root, \"benchmark_RELEASE\", \"dataset\")\n",
        "\n",
        "        for f in [\"cls\", \"img\", \"inst\", \"train.txt\", \"val.txt\"]:\n",
        "            old_path = os.path.join(extracted_ds_root, f)\n",
        "            shutil.move(old_path, sbd_root)\n",
        "        download_url(self.voc_train_url, sbd_root, self.voc_split_filename,\n",
        "                      self.voc_split_md5)\n",
        "        \n",
        "    def load_filenames(self, sbd_root, image_dir, mask_dir, image_set) -> None:\n",
        "        split_f = os.path.join(sbd_root, image_set.rstrip('\\n') + '.txt')\n",
        "\n",
        "        with open(os.path.join(split_f), \"r\") as fh:\n",
        "            file_names = [x.strip() for x in fh.readlines()]\n",
        "\n",
        "        self.images = [os.path.join(image_dir, x + \".jpg\") for x in file_names]\n",
        "        self.masks = [os.path.join(mask_dir, x + \".mat\") for x in file_names]\n",
        "        assert (len(self.images) == len(self.masks))\n",
        "        \n",
        "    # Remove images with both height and width lower than img_filter_size\n",
        "    def filter_images(self):\n",
        "        img_to_remove = set()\n",
        "        for i in range(len(self.images)):\n",
        "            img = Image.open(self.images[i])\n",
        "            width, height = img.size\n",
        "            if width < self.img_filter_size or height < self.img_filter_size:\n",
        "                img_to_remove.add(i)\n",
        "\n",
        "        self.images = [img for index, img in enumerate(self.images) if index not in img_to_remove]\n",
        "        self.masks = [mask for index, mask in enumerate(self.masks) if index not in img_to_remove]\n",
        "\n",
        "    # Remove images that dont have an instance mask\n",
        "    def remove_images_without_mask(self):\n",
        "        img_to_remove = set()\n",
        "        for i in range(len(self.images)):\n",
        "            img, target = self.__getitem__(i)\n",
        "            if torch.count_nonzero(target) < 1000:\n",
        "              img_to_remove.add(i)\n",
        "            if torch.count_nonzero(torch.logical_not(target)) < 3000:\n",
        "              img_to_remove.add(i)\n",
        "\n",
        "        self.images = [img for index, img in enumerate(self.images) if index not in img_to_remove]\n",
        "        self.masks = [mask for index, mask in enumerate(self.masks) if index not in img_to_remove]\n",
        "\n",
        "    # For targets that have multiple object instances, pick label 1 as foreground and let the others be part of the background\n",
        "    def naive_ignore_multiple_object_instances(self, target):\n",
        "        cond = torch.eq(target, torch.ones_like(target))\n",
        "        target = torch.where(cond, target, torch.zeros_like(target))\n",
        "        return target\n",
        "\n",
        "    # Apply CenterCrop to image to make it a square based on its smaller dimension\n",
        "    def center_crop(self, image, target):\n",
        "      min_len = min(image.shape[1], image.shape[2])\n",
        "      image = transforms.CenterCrop(min_len)(image)\n",
        "      target = transforms.CenterCrop(min_len)(target)\n",
        "      return image, target\n",
        "\n",
        "    # Apply Random Crop on Image & Target while keeping most of the object instance inside the crop\n",
        "    def random_crop(self, image, target):\n",
        "      left = -1\n",
        "      right = -1\n",
        "      top = -1\n",
        "      bottom = -1\n",
        "\n",
        "      # Find the extreme points of the object instance in the target\n",
        "      res = torch.nonzero(target[0])\n",
        "      values, indices = torch.min(res, 0)\n",
        "      top = values[0].item()\n",
        "      left = values[1].item()\n",
        "      values, indices = torch.max(res, 0)\n",
        "      bottom = values[0].item()\n",
        "      right = values[1].item()\n",
        "\n",
        "      # Calculate range to sample top left crop point from\n",
        "      if right - left >= self.img_filter_size:\n",
        "        x_min = (right - left)//2\n",
        "        x_max = (right - left)//2\n",
        "      else:\n",
        "        x_min = min(0, right - self.img_filter_size)\n",
        "        x_max = left\n",
        "\n",
        "      if bottom - top >= self.img_filter_size:\n",
        "        y_min = (bottom - top)//2\n",
        "        y_max = (bottom - top)//2\n",
        "      else:\n",
        "        y_min = min(0, bottom - self.img_filter_size)\n",
        "        y_max = top\n",
        "\n",
        "      # Sample top left crop point\n",
        "      x = random.randint(x_min, x_max)\n",
        "      y = random.randint(y_min, y_max)\n",
        "\n",
        "      # Apply same random crop to both image and target\n",
        "      image = transforms.functional.crop(image, y, x, self.img_filter_size, self.img_filter_size)\n",
        "      target = transforms.functional.crop(target, y, x, self.img_filter_size, self.img_filter_size)\n",
        "\n",
        "      return image, target"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137,
          "referenced_widgets": [
            "56233bc668204ee88bd60d66467862f8",
            "ffb44b1ceccd4bf1be0d8af19c7c23ec",
            "fab5b2f6ddd247bb8f972dc6c92cc0b3",
            "7f0759b66de0458691e16a27964c783e",
            "92a789bd1f5d43efa0508d8d3a197ce5",
            "2718725d50c942629f0c9c7e9a19e089",
            "294c0b4c95c04dc8a2b0b65049c7ad9e",
            "347cd3a29409469d8a5e9a2c48195482",
            "db1c28075da3434582305076998c135e",
            "d4d2c5e54ff54661b0fa95b737222966",
            "6946fa8419224c969464099429a12d2c",
            "0e708e48e971438387884c21cd3ad8fc",
            "4a68094db68d4e14b3532e0f819d2ddb",
            "d5bdbcfa9af64c44a24fd43b49c38836",
            "06e3a293a2f94bb49be2d4adb6f96af6",
            "5248fb0c55804208808e51cbfeacf57a"
          ]
        },
        "id": "-y6em226-vfx",
        "outputId": "1781add9-0d42-45f9-bcec-c46275a0e1d2"
      },
      "source": [
        "# Download the training set\n",
        "sbd = CustomSBDataset(root=\".\", image_set=\"train\", mode=\"segmentation\", download=False)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/semantic_contours/benchmark.tgz to ./benchmark.tgz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56233bc668204ee88bd60d66467862f8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://home.bharathh.info/pubs/codes/SBD/train_noval.txt to ./train_noval.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db1c28075da3434582305076998c135e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W19rDu1Q5-o1"
      },
      "source": [
        "# Download the validation Dataset\n",
        "sbd_val = CustomSBDataset(root=\".\", image_set=\"val\", mode=\"segmentation\", download=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kaV9X_w-3GK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4feba297-5927-4156-89f1-fad5440deca9"
      },
      "source": [
        "# Check img and target shape\n",
        "img, target = sbd.__getitem__(0)\n",
        "print(img.shape, target.shape, sbd.__len__())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 480, 480]) torch.Size([1, 480, 480]) 7993\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_qE74Q06PNt"
      },
      "source": [
        "# Code to display image and segmentation mask. Set images to be num of image, segmentation masks to display\n",
        "images = 10\n",
        "fig,axes = plt.subplots(nrows = images, ncols = 2, figsize=(50,50))\n",
        "\n",
        "for i in range(images):\n",
        "    img, target = sbd.__getitem__(i)\n",
        "    axes[i,0].imshow(transforms.ToPILImage()(img))\n",
        "    axes[i,1].imshow(transforms.ToPILImage()(target))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yz9qkVk7CZog"
      },
      "source": [
        "# Sample user clicks using target\n",
        "def gen_clicks(target):\n",
        "  num_pos = random.randint(1, 10)\n",
        "  num_neg = random.randint(0, 10)\n",
        "\n",
        "  pos_clicks = gen_pos_clicks(target, num_pos)\n",
        "  neg_clicks = gen_neg_clicks(target, num_neg)\n",
        "\n",
        "  return pos_clicks, neg_clicks\n",
        "\n",
        "# Generate n +ve clicks by randomly sampling points in the foreground\n",
        "def gen_pos_clicks(target, n):\n",
        "  pos_clicks = torch.empty((0, 2))\n",
        "  dstep = 5\n",
        "  dmargin = 5\n",
        "\n",
        "  distances = ndimage.distance_transform_edt(target[0].detach().cpu().numpy())\n",
        "  distances = np.where(distances < dmargin, 0, distances)\n",
        "  points = torch.nonzero(torch.from_numpy(distances))\n",
        "  if len(points) == 0:\n",
        "    print(\"Pos: {}\".format(torch.count_nonzero(target[0])))\n",
        "\n",
        "  for i in range(n):\n",
        "    resample = 1\n",
        "    while (resample > 0 and resample < 4):\n",
        "      index = random.randint(0, len(points)-1)\n",
        "      point = points[index]\n",
        "      if len(pos_clicks) == 0:\n",
        "        pos_clicks = torch.cat((pos_clicks, point.unsqueeze(0)))\n",
        "        resample = 0\n",
        "        break\n",
        "      else:\n",
        "        min_dist = dstep\n",
        "        for pos in pos_clicks:\n",
        "          dist = torch.dist(point.type(torch.FloatTensor), pos.type(torch.FloatTensor), 2)\n",
        "          if dist < min_dist:\n",
        "            min_dist = dist\n",
        "            break\n",
        "        if min_dist >= dstep:\n",
        "          pos_clicks = torch.cat((pos_clicks, point.unsqueeze(0)))\n",
        "          resample = 0\n",
        "          break\n",
        "        resample += 1\n",
        "\n",
        "  return pos_clicks\n",
        "\n",
        "# Generate n -ve clicks by randomly sampling points in the background\n",
        "def gen_neg_clicks(target, n):\n",
        "  neg_clicks = torch.empty((0, 2))\n",
        "  dstep = 5\n",
        "  dmargin_min = 5\n",
        "  dmargin_max = 50\n",
        "\n",
        "  target_inv = torch.logical_not(target[0])\n",
        "  distances = ndimage.distance_transform_edt(target_inv.detach().cpu().numpy())\n",
        "  distances = np.where(distances < dmargin_min, 0, distances)\n",
        "  distances = np.where(distances > dmargin_max, 0, distances)\n",
        "  points = torch.nonzero(torch.from_numpy(distances))\n",
        "  if len(points) == 0:\n",
        "    print(\"Neg: {}\".format(torch.count_nonzero(target_inv)))\n",
        "\n",
        "  for i in range(n):\n",
        "    resample = 1\n",
        "    while (resample > 0 and resample < 4):\n",
        "      index = random.randint(0, len(points)-1)\n",
        "      point = points[index]\n",
        "      if len(neg_clicks) == 0:\n",
        "        neg_clicks = torch.cat((neg_clicks, point.unsqueeze(0)))\n",
        "        resample = 0\n",
        "        break\n",
        "      else:\n",
        "        min_dist = dstep\n",
        "        for neg in neg_clicks:\n",
        "          dist = torch.dist(point.type(torch.FloatTensor), neg.type(torch.FloatTensor), 2)\n",
        "          if dist < min_dist:\n",
        "            min_dist = dist\n",
        "            break\n",
        "        if min_dist >= dstep:\n",
        "          neg_clicks = torch.cat((neg_clicks, point.unsqueeze(0)))\n",
        "          resample = 0\n",
        "          break\n",
        "        resample += 1\n",
        "\n",
        "  return neg_clicks"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ5YevAVas2t"
      },
      "source": [
        "# Sample user clicks using target\n",
        "def gen_clicks_2(target):\n",
        "  num_pos = random.randint(1, 10)\n",
        "  num_neg = random.randint(0, 10)\n",
        "  pos_clicks = torch.empty((0, 2))\n",
        "  neg_clicks = torch.empty((0, 2))\n",
        "\n",
        "  for i in range(num_pos):\n",
        "    pos_clicks = gen_pos_click(target, pos_clicks)\n",
        "  for i in range(num_neg):\n",
        "    neg_clicks = gen_neg_click(target, neg_clicks)\n",
        "\n",
        "  return pos_clicks, neg_clicks\n",
        "\n",
        "# Generates a +ve clicks by randomly sampling points in the foreground\n",
        "def gen_pos_click(target, pos_clicks):\n",
        "  dstep = 5\n",
        "  dmargin = 5\n",
        "\n",
        "  distances = ndimage.distance_transform_edt(target[0].detach().cpu().numpy())\n",
        "  distances = np.where(distances < dmargin, 0, distances)\n",
        "  points = torch.nonzero(torch.from_numpy(distances))\n",
        "  if len(points) == 0:\n",
        "    print(\"Pos: {}\".format(torch.count_nonzero(target[0])))\n",
        "\n",
        "  resample = 1\n",
        "  while (resample > 0 and resample < 4):\n",
        "    index = random.randint(0, len(points)-1)\n",
        "    point = points[index]\n",
        "    if len(pos_clicks) == 0:\n",
        "      pos_clicks = torch.cat((pos_clicks, point.unsqueeze(0)))\n",
        "      resample = 0\n",
        "      break\n",
        "    else:\n",
        "      min_dist = dstep\n",
        "      for pos in pos_clicks:\n",
        "        dist = torch.dist(point.type(torch.FloatTensor), pos.type(torch.FloatTensor), 2)\n",
        "        if dist < min_dist:\n",
        "          min_dist = dist\n",
        "          break\n",
        "      if min_dist >= dstep:\n",
        "        pos_clicks = torch.cat((pos_clicks, point.unsqueeze(0)))\n",
        "        resample = 0\n",
        "        break\n",
        "      resample += 1\n",
        "\n",
        "  return pos_clicks\n",
        "\n",
        "# Generates a -ve click by randomly sampling points in the background\n",
        "def gen_neg_click(target, neg_clicks):\n",
        "  dstep = 5\n",
        "  dmargin_min = 5\n",
        "  dmargin_max = 50\n",
        "\n",
        "  target_inv = torch.logical_not(target[0])\n",
        "  distances = ndimage.distance_transform_edt(target_inv.detach().cpu().numpy())\n",
        "  distances = np.where(distances < dmargin_min, 0, distances)\n",
        "  distances = np.where(distances > dmargin_max, 0, distances)\n",
        "  points = torch.nonzero(torch.from_numpy(distances))\n",
        "  if len(points) == 0:\n",
        "    print(\"Neg: {}\".format(torch.count_nonzero(target_inv)))\n",
        "\n",
        "  resample = 1\n",
        "  while (resample > 0 and resample < 4):\n",
        "    index = random.randint(0, len(points)-1)\n",
        "    point = points[index]\n",
        "    if len(neg_clicks) == 0:\n",
        "      neg_clicks = torch.cat((neg_clicks, point.unsqueeze(0)))\n",
        "      resample = 0\n",
        "      break\n",
        "    else:\n",
        "      min_dist = dstep\n",
        "      for neg in neg_clicks:\n",
        "        dist = torch.dist(point.type(torch.FloatTensor), neg.type(torch.FloatTensor), 2)\n",
        "        if dist < min_dist:\n",
        "          min_dist = dist\n",
        "          break\n",
        "      if min_dist >= dstep:\n",
        "        neg_clicks = torch.cat((neg_clicks, point.unsqueeze(0)))\n",
        "        resample = 0\n",
        "        break\n",
        "      resample += 1\n",
        "\n",
        "  return neg_clicks"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNOxZdX7vz13"
      },
      "source": [
        "\"\"\"\n",
        "Convert clicks to interaction maps\n",
        "Input:\n",
        "  pos_clicks: list of x,y +ve click locations \n",
        "  neg_clicks: list of x,y -ve click locations\n",
        "Output:\n",
        "  interaction_map: Concatenation of +ve & -ve maps\n",
        "  click_location: Tensor with 1's wherever there is a click\n",
        "  click_type: Tensor with 1 wherever there is a +ve click and 0 wherever there is a -ve click\n",
        "\"\"\" \n",
        "def convert_clicks(pos_clicks, neg_clicks):\n",
        "  pos_map = torch.ones((480, 480))\n",
        "  neg_map = torch.ones((480, 480))\n",
        "  click_location = torch.zeros((480, 480))\n",
        "  click_type = torch.zeros((480, 480))\n",
        "\n",
        "  for click in pos_clicks:\n",
        "    pos_map[int(click[0])][int(click[1])] = 0\n",
        "    click_location[int(click[0])][int(click[1])] = 1\n",
        "    click_type[int(click[0])][int(click[1])] = 1\n",
        "  pos_map = ndimage.distance_transform_edt(pos_map.detach().cpu().numpy())\n",
        "  pos_map = np.where(pos_map > 255, 255, pos_map)\n",
        "  pos_map = torch.from_numpy(pos_map).unsqueeze(0)\n",
        "  \n",
        "  if len(neg_clicks) > 0:\n",
        "    for click in neg_clicks:\n",
        "      neg_map[int(click[0])][int(click[1])] = 0\n",
        "      click_location[int(click[0])][int(click[1])] = 1\n",
        "      click_type[int(click[0])][int(click[1])] = 0\n",
        "    neg_map = ndimage.distance_transform_edt(neg_map.detach().cpu().numpy())\n",
        "    neg_map = np.where(neg_map > 255, 255, neg_map)\n",
        "    neg_map = torch.from_numpy(neg_map).unsqueeze(0)\n",
        "  else:\n",
        "    neg_map = neg_map*255\n",
        "    neg_map = neg_map.unsqueeze(0)\n",
        "\n",
        "  click_location = click_location.unsqueeze(0)\n",
        "  click_type = click_type.unsqueeze(0)\n",
        "  interaction_map = torch.cat((pos_map, neg_map), 0)\n",
        "\n",
        "  return interaction_map, click_location, click_type"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idazCHrFCiU3",
        "outputId": "da0b79c5-947b-4dc5-fb83-8d1e1b9e7f56"
      },
      "source": [
        "# Code to test click generation by ploting clicks and segmentation mask\n",
        "img, target = sbd.__getitem__(4)\n",
        "pos_clicks = gen_pos_clicks(target, 5)\n",
        "print(pos_clicks)\n",
        "neg_clicks = gen_neg_clicks(target, 3)\n",
        "print(neg_clicks)\n",
        "plt.imshow(transforms.ToPILImage()(target))\n",
        "plt.scatter(pos_clicks[:, 1], pos_clicks[:, 0])\n",
        "plt.scatter(neg_clicks[:, 1], neg_clicks[:, 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[137., 257.],\n",
            "        [103., 412.],\n",
            "        [225., 344.],\n",
            "        [ 37., 433.],\n",
            "        [ 75., 351.]])\n",
            "tensor([[388., 363.],\n",
            "        [249.,  64.],\n",
            "        [253., 393.]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f858c7de640>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FpeQGf_v73r",
        "outputId": "dea55da4-71f9-4196-f812-d365b339edcf"
      },
      "source": [
        "# Code to visualize interaction maps\n",
        "iact_map, click_location, click_type = convert_clicks(pos_clicks, neg_clicks)\n",
        "fig,axes = plt.subplots(nrows = 1, ncols = 2)\n",
        "axes[0].imshow(iact_map[0].numpy(), cmap='gray', interpolation='bicubic', vmin=0, vmax=255)\n",
        "axes[1].imshow(iact_map[1].numpy(), cmap='gray', interpolation='bicubic', vmin=0, vmax=255)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dKzk186FYEV"
      },
      "source": [
        "\"\"\"\n",
        "DenseNet Encoder Network\n",
        "\"\"\"\n",
        "\n",
        "model_urls = {\n",
        "    'densenet121': 'https://download.pytorch.org/models/densenet121-a639ec97.pth',\n",
        "    'densenet169': 'https://download.pytorch.org/models/densenet169-b2777c0a.pth',\n",
        "    'densenet201': 'https://download.pytorch.org/models/densenet201-c1103571.pth',\n",
        "    'densenet161': 'https://download.pytorch.org/models/densenet161-8d451a50.pth',\n",
        "}\n",
        "\n",
        "BOTTLENECK_WIDTH = 128\n",
        "COEFF = 12.0\n",
        "\n",
        "def lip2d(x, logit, kernel=3, stride=2, padding=1):\n",
        "    weight = logit.exp()\n",
        "    return F.avg_pool2d(x*weight, kernel, stride, padding)/F.avg_pool2d(weight, kernel, stride, padding)\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class SoftGate(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return torch.sigmoid(x).mul(COEFF)\n",
        "\n",
        "\n",
        "class BottleneckLIP(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(BottleneckLIP, self).__init__()\n",
        "\n",
        "        rp = BOTTLENECK_WIDTH\n",
        "\n",
        "        self.logit = nn.Sequential(\n",
        "            OrderedDict((\n",
        "                ('conv1', conv1x1(channels, rp)),\n",
        "                ('bn1', nn.InstanceNorm2d(rp, affine=True)),\n",
        "                ('relu1', nn.ReLU(inplace=True)),\n",
        "                ('conv2', conv3x3(rp, rp)),\n",
        "                ('bn2', nn.InstanceNorm2d(rp, affine=True)),\n",
        "                ('relu2', nn.ReLU(inplace=True)),\n",
        "                ('conv3', conv1x1(rp, channels)),\n",
        "                ('bn3', nn.InstanceNorm2d(channels, affine=True)),\n",
        "                ('gate', SoftGate()),\n",
        "            ))\n",
        "        )\n",
        "\n",
        "    def init_layer(self):\n",
        "        self.logit[6].weight.data.fill_(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        frac = lip2d(x, self.logit(x), kernel=2, stride=2, padding=0)\n",
        "        return frac\n",
        "\n",
        "\n",
        "class SimplifiedLIP(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(SimplifiedLIP, self).__init__()\n",
        "\n",
        "        self.logit = nn.Sequential(\n",
        "            OrderedDict((\n",
        "                ('conv1', conv3x3(channels, channels)),\n",
        "                ('bn1', nn.InstanceNorm2d(channels, affine=True)),\n",
        "                ('gate', SoftGate()),\n",
        "            ))\n",
        "        )\n",
        "\n",
        "    def init_layer(self):\n",
        "        self.logit[0].weight.data.fill_(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        frac = lip2d(x, self.logit(x), kernel=3, stride=2, padding=1)\n",
        "        return frac\n",
        "\n",
        "\n",
        "class ASPP(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(ASPP, self).__init__()\n",
        "\n",
        "      self.conv_1x1_1 = nn.Conv2d(1024, 256, kernel_size=1)\n",
        "      self.bn_conv_1x1_1 = nn.BatchNorm2d(256)\n",
        "\n",
        "      self.conv_3x3_1 = nn.Conv2d(1024, 256, kernel_size=3, stride=1, padding=6, dilation=6)\n",
        "      self.bn_conv_3x3_1 = nn.BatchNorm2d(256)\n",
        "\n",
        "      self.conv_3x3_2 = nn.Conv2d(1024, 256, kernel_size=3, stride=1, padding=12, dilation=12)\n",
        "      self.bn_conv_3x3_2 = nn.BatchNorm2d(256)\n",
        "\n",
        "      self.conv_3x3_3 = nn.Conv2d(1024, 256, kernel_size=3, stride=1, padding=18, dilation=18)\n",
        "      self.bn_conv_3x3_3 = nn.BatchNorm2d(256)\n",
        "\n",
        "      self.conv_3x3_4 = nn.Conv2d(1024, 256, kernel_size=3, stride=1, padding=24, dilation=24)\n",
        "      self.bn_conv_3x3_4 = nn.BatchNorm2d(256)\n",
        "\n",
        "      self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "      self.conv_1x1_2 = nn.Conv2d(1024, 256, kernel_size=1)\n",
        "      self.bn_conv_1x1_2 = nn.BatchNorm2d(256)\n",
        "\n",
        "      self.conv_1x1_3 = nn.Conv2d(1280, 256, kernel_size=1)\n",
        "      self.bn_conv_1x1_3 = nn.BatchNorm2d(256)\n",
        "\n",
        "      self.conv_1x1_4 = nn.Conv2d(256, 1024, kernel_size=1)\n",
        "\n",
        "    def forward(self, feature_map):\n",
        "      # feature_map from DenseBlock4 has shape (batch_size, 1024, 15, 15)\n",
        "\n",
        "      feature_map_h = feature_map.size()[2]\n",
        "      feature_map_w = feature_map.size()[3] \n",
        "\n",
        "      out_1x1 = F.relu(self.bn_conv_1x1_1(self.conv_1x1_1(feature_map))) # (shape: (batch_size, 256, 15, 15))\n",
        "      out_3x3_1 = F.relu(self.bn_conv_3x3_1(self.conv_3x3_1(feature_map))) # (shape: (batch_size, 256, 15, 15))\n",
        "      out_3x3_2 = F.relu(self.bn_conv_3x3_2(self.conv_3x3_2(feature_map))) # (shape: (batch_size, 256, 15, 15))\n",
        "      out_3x3_3 = F.relu(self.bn_conv_3x3_3(self.conv_3x3_3(feature_map))) # (shape: (batch_size, 256, 15, 15))\n",
        "      #out_3x3_4 = F.relu(self.bn_conv_3x3_4(self.conv_3x3_4(feature_map))) # (shape: (batch_size, 256, 15, 15))\n",
        "\n",
        "      out_img = self.avg_pool(feature_map) # (shape: (batch_size, 1024, 1, 1))\n",
        "      #out_img = F.relu(self.bn_conv_1x1_2(self.conv_1x1_2(out_img))) # (shape: (batch_size, 256, 1, 1))\n",
        "      out_img = F.relu(self.conv_1x1_2(out_img)) # (shape: (batch_size, 256, 1, 1))\n",
        "      out_img = F.interpolate(out_img, size=(feature_map_h, feature_map_w), mode=\"bilinear\") # (shape: (batch_size, 256, 15, 15))\n",
        "\n",
        "      out = torch.cat([out_1x1, out_3x3_1, out_3x3_2, out_3x3_3, out_img], 1) # (shape: (batch_size, 1280, 15, 15))\n",
        "      out = F.relu(self.bn_conv_1x1_3(self.conv_1x1_3(out))) # (shape: (batch_size, 256, 15, 15))\n",
        "      out = self.conv_1x1_4(out) # (shape: (batch_size, 1024, 15, 15))\n",
        "\n",
        "      return out\n",
        "\n",
        "\n",
        "class SemanticSupervisionBlock(nn.Module):\n",
        "    def __init__(self, channels, num_classes):\n",
        "      super(SemanticSupervisionBlock, self).__init__()\n",
        "      self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, stride=2, padding=1)\n",
        "      self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=2, padding=1)\n",
        "      self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "      self.linear1 = nn.Linear(channels, channels//2)\n",
        "      self.linear2 = nn.Linear(channels//2, num_classes)\n",
        "\n",
        "    def forward(self, feature_map):\n",
        "      out = self.conv1(feature_map)\n",
        "      out = self.conv2(out)\n",
        "      out = self.avg_pool(out)\n",
        "      out = torch.flatten(out, 1)\n",
        "      out = self.linear1(out)\n",
        "      out = self.linear2(out)\n",
        "      return out\n",
        "\n",
        "\n",
        "class _DenseLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_input_features: int,\n",
        "        growth_rate: int,\n",
        "        bn_size: int,\n",
        "        drop_rate: float\n",
        "    ) -> None:\n",
        "        super(_DenseLayer, self).__init__()\n",
        "        self.norm1: nn.BatchNorm2d\n",
        "        self.add_module('norm1', nn.BatchNorm2d(num_input_features))\n",
        "        self.relu1: nn.ReLU\n",
        "        self.add_module('relu1', nn.ReLU(inplace=True))\n",
        "        self.conv1: nn.Conv2d\n",
        "        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\n",
        "                                           growth_rate, kernel_size=1, stride=1,\n",
        "                                           bias=False))\n",
        "        self.norm2: nn.BatchNorm2d\n",
        "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate))\n",
        "        self.relu2: nn.ReLU\n",
        "        self.add_module('relu2', nn.ReLU(inplace=True))\n",
        "        self.conv2: nn.Conv2d\n",
        "        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
        "                                           kernel_size=3, stride=1, padding=1,\n",
        "                                           bias=False))\n",
        "        self.drop_rate = float(drop_rate)\n",
        "\n",
        "    def bn_function(self, inputs: List[Tensor]) -> Tensor:\n",
        "        concated_features = torch.cat(inputs, 1)\n",
        "        bottleneck_output = self.conv1(self.relu1(self.norm1(concated_features)))\n",
        "        return bottleneck_output\n",
        "\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        if isinstance(input, Tensor):\n",
        "            prev_features = [input]\n",
        "        else:\n",
        "            prev_features = input\n",
        "\n",
        "        bottleneck_output = self.bn_function(prev_features)\n",
        "\n",
        "        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n",
        "        if self.drop_rate > 0:\n",
        "            new_features = F.dropout(new_features, p=self.drop_rate,\n",
        "                                     training=self.training)\n",
        "        return new_features\n",
        "\n",
        "\n",
        "class _DenseBlock(nn.ModuleDict):\n",
        "    _version = 2\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_layers: int,\n",
        "        num_input_features: int,\n",
        "        bn_size: int,\n",
        "        growth_rate: int,\n",
        "        drop_rate: float\n",
        "    ) -> None:\n",
        "        super(_DenseBlock, self).__init__()\n",
        "        for i in range(num_layers):\n",
        "            layer = _DenseLayer(\n",
        "                num_input_features + i * growth_rate,\n",
        "                growth_rate=growth_rate,\n",
        "                bn_size=bn_size,\n",
        "                drop_rate=drop_rate,\n",
        "            )\n",
        "            self.add_module('denselayer%d' % (i + 1), layer)\n",
        "\n",
        "    def forward(self, init_features: Tensor) -> Tensor:\n",
        "        features = [init_features]\n",
        "        for name, layer in self.items():\n",
        "            new_features = layer(features)\n",
        "            features.append(new_features)\n",
        "        return torch.cat(features, 1)\n",
        "\n",
        "\n",
        "class _Transition(nn.Sequential):\n",
        "    def __init__(self, num_input_features: int, num_output_features: int, include_lip: bool) -> None:\n",
        "        super(_Transition, self).__init__()\n",
        "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n",
        "        self.add_module('relu', nn.ReLU(inplace=True))\n",
        "        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n",
        "                                          kernel_size=1, stride=1, bias=False))\n",
        "        if include_lip:\n",
        "          self.add_module('pool', BottleneckLIP(num_output_features))\n",
        "        else:\n",
        "          self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "    \"\"\"Densenet-BC model class, based on\n",
        "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_.\n",
        "    Args:\n",
        "        growth_rate (int) - how many filters to add each layer (`k` in paper)\n",
        "        block_config (list of 4 ints) - how many layers in each pooling block\n",
        "        num_init_features (int) - the number of filters to learn in the first convolution layer\n",
        "        bn_size (int) - multiplicative factor for number of bottle neck layers\n",
        "          (i.e. bn_size * k features in the bottleneck layer)\n",
        "        drop_rate (float) - dropout rate after each dense layer\n",
        "        num_classes (int) - number of classification classes\n",
        "        include_aspp (bool) - Use ASPP block\n",
        "        include_lip (bool) - Use LIP\n",
        "        include_ss (bool) - Use Semantic Supervision Block\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        growth_rate: int = 32,\n",
        "        block_config: Tuple[int, int, int, int] = (6, 12, 24, 16),\n",
        "        num_init_features: int = 64,\n",
        "        bn_size: int = 4,\n",
        "        drop_rate: float = 0,\n",
        "        num_classes: int = 1000,\n",
        "        include_aspp: bool = False,\n",
        "        include_lip: bool = False,\n",
        "        include_ss: bool = False\n",
        "    ) -> None:\n",
        "\n",
        "        super(DenseNet, self).__init__()\n",
        "        self.include_aspp = include_aspp\n",
        "        self.include_ss = include_ss\n",
        "\n",
        "        # First convolution\n",
        "        if include_lip:\n",
        "          self.features = nn.Sequential(OrderedDict([\n",
        "              ('conv0', nn.Conv2d(5, num_init_features,\n",
        "                                  kernel_size=7, stride=2, padding=3, bias=False)),\n",
        "              ('norm0', nn.BatchNorm2d(num_init_features)),\n",
        "              ('relu0', nn.ReLU(inplace=True)),\n",
        "              ('pool0', SimplifiedLIP(num_init_features)),\n",
        "          ]))\n",
        "\n",
        "        else:\n",
        "          self.features = nn.Sequential(OrderedDict([\n",
        "              ('conv0', nn.Conv2d(5, num_init_features, kernel_size=7, stride=2,\n",
        "                                  padding=3, bias=False)),\n",
        "              ('norm0', nn.BatchNorm2d(num_init_features)),\n",
        "              ('relu0', nn.ReLU(inplace=True)),\n",
        "              ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
        "          ]))\n",
        "\n",
        "        # Each denseblock\n",
        "        num_features = num_init_features\n",
        "        for i, num_layers in enumerate(block_config):\n",
        "            block = _DenseBlock(\n",
        "                num_layers=num_layers,\n",
        "                num_input_features=num_features,\n",
        "                bn_size=bn_size,\n",
        "                growth_rate=growth_rate,\n",
        "                drop_rate=drop_rate\n",
        "            )\n",
        "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
        "            num_features = num_features + num_layers * growth_rate\n",
        "            if i != len(block_config) - 1:\n",
        "                trans = _Transition(num_input_features=num_features,\n",
        "                                    num_output_features=num_features // 2, include_lip=include_lip)\n",
        "                self.features.add_module('transition%d' % (i + 1), trans)\n",
        "                num_features = num_features // 2\n",
        "\n",
        "        # Semantic Supervision Blocks\n",
        "        self.ss1 = SemanticSupervisionBlock(256, num_classes)\n",
        "        self.ss2 = SemanticSupervisionBlock(512, num_classes)\n",
        "        self.ss3 = SemanticSupervisionBlock(1024, num_classes)\n",
        "        \n",
        "        # ASPP Block\n",
        "        self.aspp = ASPP()\n",
        "\n",
        "        # Final batch norm\n",
        "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
        "\n",
        "        # Linear layer\n",
        "        self.classifier = nn.Linear(num_features, num_classes)\n",
        "\n",
        "        # Official init from torch repo.\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # First Convolution\n",
        "        x = self.features.conv0(x)\n",
        "        x = self.features.norm0(x)\n",
        "        x = self.features.relu0(x)\n",
        "        x = self.features.pool0(x)\n",
        "\n",
        "        # Dense Blocks\n",
        "        dense1_out = self.features.denseblock1(x)\n",
        "        x = self.features.transition1(dense1_out)\n",
        "        dense2_out = self.features.denseblock2(x)\n",
        "        x = self.features.transition2(dense2_out)\n",
        "        dense3_out = self.features.denseblock3(x)\n",
        "        x = self.features.transition3(dense3_out)\n",
        "        dense4_out = self.features.denseblock4(x)\n",
        "        \n",
        "        # Semantic Supervision\n",
        "        if self.include_ss:\n",
        "          ss1_out = self.ss1(dense1_out)\n",
        "          ss2_out = self.ss2(dense2_out)\n",
        "          ss3_out = self.ss3(dense3_out)\n",
        "\n",
        "        # ASPP\n",
        "        if self.include_aspp:\n",
        "          dense4_out = self.aspp(dense4_out);\n",
        "\n",
        "        out = self.features.norm5(dense4_out)\n",
        "        out = F.relu(out, inplace=True)\n",
        "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.classifier(out)\n",
        "\n",
        "        dense_out = [dense1_out, dense2_out, dense3_out, dense4_out]\n",
        "        if self.include_ss:\n",
        "          ss_out = [ss1_out, ss2_out, ss3_out]\n",
        "        else:\n",
        "          ss_out = []\n",
        "        return out, dense_out, ss_out\n",
        "\n",
        "\n",
        "def _load_state_dict(model: nn.Module, model_url: str, progress: bool) -> None:\n",
        "    # '.'s are no longer allowed in module names, but previous _DenseLayer\n",
        "    # has keys 'norm.1', 'relu.1', 'conv.1', 'norm.2', 'relu.2', 'conv.2'.\n",
        "    # They are also in the checkpoints in model_urls. This pattern is used\n",
        "    # to find such keys.\n",
        "    pattern = re.compile(\n",
        "        r'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$')\n",
        "\n",
        "    state_dict = load_state_dict_from_url(model_url, progress=progress)\n",
        "    for key in list(state_dict.keys()):\n",
        "        res = pattern.match(key)\n",
        "        if res:\n",
        "            new_key = res.group(1) + res.group(2)\n",
        "            state_dict[new_key] = state_dict[key]\n",
        "            del state_dict[key]\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "\n",
        "def _densenet(\n",
        "    arch: str,\n",
        "    growth_rate: int,\n",
        "    block_config: Tuple[int, int, int, int],\n",
        "    num_init_features: int,\n",
        "    pretrained: bool,\n",
        "    progress: bool,\n",
        "    include_aspp: bool,\n",
        "    include_lip: bool,\n",
        "    include_ss: bool,\n",
        "    **kwargs: Any\n",
        ") -> DenseNet:\n",
        "    model = DenseNet(growth_rate, block_config, num_init_features, include_aspp=include_aspp, include_lip=include_lip, include_ss=include_ss, **kwargs)\n",
        "    if pretrained:\n",
        "        _load_state_dict(model, model_urls[arch], progress)\n",
        "    return model\n",
        "\n",
        "\n",
        "def densenet121(pretrained: bool = False, progress: bool = True, include_aspp: bool = False, include_lip: bool = False, include_ss: bool = False, **kwargs: Any) -> DenseNet:\n",
        "    r\"\"\"Densenet-121 model from\n",
        "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n",
        "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_.\n",
        "    \"\"\"\n",
        "    return _densenet('densenet121', 32, (6, 12, 24, 16), 64, pretrained, progress, include_aspp, include_lip, include_ss, **kwargs)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqUPHUtsIsKE"
      },
      "source": [
        "\"\"\"\n",
        "Decoder Network\n",
        "\"\"\"\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.conv_SE_1_32_1 = nn.Conv2d(in_channels=1024, out_channels=64, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=True)\n",
        "    self.conv_SE_1_32_2 = nn.Conv2d(in_channels=64, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=True)\n",
        "    self.conv_1_32_1d = nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=False)\n",
        "    self.prelu_1_32_1d = nn.PReLU(num_parameters=512)\n",
        "    self.bn_1_32_1d = nn.BatchNorm2d(num_features=512, eps=9.999999747378752e-06, momentum=0.0)\n",
        "    self.conv_1_32_2d = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=False)\n",
        "    self.prelu_1_32_2d = nn.PReLU(num_parameters=512)\n",
        "    self.bn_1_32_2d = nn.BatchNorm2d(num_features=512, eps=9.999999747378752e-06, momentum=0.0)\n",
        "    self.conv_1_32_3d = nn.Conv2d(in_channels=512, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)\n",
        "    self.prelu_1_32_3d = nn.PReLU(num_parameters=256)\n",
        "    self.bn_1_32_3d = nn.BatchNorm2d(num_features=256, eps=9.999999747378752e-06, momentum=0.0)\n",
        "    self.deconv_1_16d = nn.ConvTranspose2d(in_channels=256, out_channels=256, kernel_size=2, stride=2, bias=False)\n",
        "    self.prelu_1_16d = nn.PReLU(num_parameters=256)\n",
        "    self.bn_1_16d = nn.BatchNorm2d(num_features=256, eps=9.999999747378752e-06, momentum=0.0)\n",
        "    self.conv_SE_1_16_1 = nn.Conv2d(in_channels=1024, out_channels=64, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)\n",
        "    self.conv_SE_1_16_2 = nn.Conv2d(in_channels=64, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)\n",
        "    self.conv_SE_1_16 = nn.Conv2d(in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)\n",
        "    self.prelu_SE_1_16 = nn.PReLU(num_parameters=256)\n",
        "    self.bn_SE_1_16 = nn.BatchNorm2d(num_features=256, eps=9.999999747378752e-06, momentum=0.0)\n",
        "\n",
        "    self.conv_1_16_1d = nn.Conv2d(in_channels=512, out_channels=256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=False)\n",
        "    self.prelu_1_16_1d = nn.PReLU(num_parameters=256)\n",
        "    self.bn_1_16_1d = nn.BatchNorm2d(num_features=256, eps=9.999999747378752e-06, momentum=0.0)\n",
        "    self.conv_1_16_2d = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=False)\n",
        "    self.prelu_1_16_2d = nn.PReLU(num_parameters=256)\n",
        "    self.bn_1_16_2d = nn.BatchNorm2d(num_features=256, eps=9.999999747378752e-06, momentum=0.0)\n",
        "    self.conv_1_16_3d = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)\n",
        "    self.prelu_1_16_3d = nn.PReLU(num_parameters=128)\n",
        "    self.bn_1_16_3d = nn.BatchNorm2d(num_features=128, eps=9.999999747378752e-06, momentum=0.0)\n",
        "    self.deconv_1_8d = nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=2, stride=2, bias=False)\n",
        "    self.prelu_1_8d = nn.PReLU(num_parameters=128)\n",
        "    self.bn_1_8d = nn.BatchNorm2d(num_features=128, eps=9.999999747378752e-06, momentum=0.0)\n",
        "    self.conv_SE_1_8_1 = nn.Conv2d(in_channels=512, out_channels=32, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)\n",
        "    self.conv_SE_1_8_2 = nn.Conv2d(in_channels=32, out_channels=512, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)\n",
        "    self.conv_SE_1_8 = nn.Conv2d(in_channels=512, out_channels=128, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)\n",
        "    self.prelu_SE_1_8 = nn.PReLU(num_parameters=128)\n",
        "    self.bn_SE_1_8 = nn.BatchNorm2d(num_features=128, eps=9.999999747378752e-06, momentum=0.0)\n",
        "\n",
        "    self.conv_1_8_1d = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=False)\n",
        "    self.prelu_1_8_1d = nn.PReLU(num_parameters=128)\n",
        "    self.bn_1_8_1d = nn.BatchNorm2d(num_features=128, eps=9.999999747378752e-06, momentum=0.0)\n",
        "    self.conv_1_8_2d = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=False)\n",
        "    self.prelu_1_8_2d = nn.PReLU(num_parameters=128)\n",
        "    self.bn_1_8_2d = nn.BatchNorm2d(num_features=128, eps=9.999999747378752e-06, momentum=0.0)\n",
        "    self.conv_1_8_3d = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)\n",
        "    self.prelu_1_8_3d = nn.PReLU(num_parameters=64)\n",
        "    self.bn_1_8_3d = nn.BatchNorm2d(num_features=64, eps=9.999999747378752e-06, momentum=0.0)\n",
        "    self.deconv_1_4d = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=2, stride=2, bias=False)\n",
        "    self.prelu_1_4d = nn.PReLU(num_parameters=64)\n",
        "    self.bn_1_4d = nn.BatchNorm2d(num_features=64, eps=9.999999747378752e-06, momentum=0.0)\n",
        "    self.conv_SE_1_4_1 = nn.Conv2d(in_channels=256, out_channels=16, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)\n",
        "    self.conv_SE_1_4_2 = nn.Conv2d(in_channels=16, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)\n",
        "    self.conv_SE_1_4 = nn.Conv2d(in_channels=256, out_channels=64, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)\n",
        "    self.prelu_SE_1_4 = nn.PReLU(num_parameters=64)\n",
        "    self.bn_SE_1_4 = nn.BatchNorm2d(num_features=64, eps=9.999999747378752e-06, momentum=0.0)\n",
        "\n",
        "    self.conv_1_4_1d = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=False)\n",
        "    self.prelu_1_4_1d = nn.PReLU(num_parameters=64)\n",
        "    self.bn_1_4_1d = nn.BatchNorm2d(num_features=64, eps=9.999999747378752e-06, momentum=0.0)\n",
        "    self.conv_1_4_2d = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=False)\n",
        "    self.prelu_1_4_2d = nn.PReLU(num_parameters=64)\n",
        "    self.bn_1_4_2d = nn.BatchNorm2d(num_features=64, eps=9.999999747378752e-06, momentum=0.0)\n",
        "    self.conv_1_4_3d = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)\n",
        "    self.prelu_1_4_3d = nn.PReLU(num_parameters=32)\n",
        "    self.bn_1_4_3d = nn.BatchNorm2d(num_features=32, eps=9.999999747378752e-06, momentum=0.0)\n",
        "\n",
        "    self.pred_1_4 = nn.Conv2d(in_channels=32, out_channels=1, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=True)\n",
        "    self.pred_step_1 = nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=4, stride=4, bias=False)\n",
        "\n",
        "    self.conv_atrous1_1 = nn.Conv2d(in_channels=6, out_channels=32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=False)\n",
        "    self.prelu_atrous1_1 = nn.PReLU(num_parameters=32)\n",
        "    self.bn_atrous1_1 = nn.BatchNorm2d(num_features=32, eps=9.999999747378752e-06, momentum=0.0)\n",
        "    self.conv_atrous1_2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=False)\n",
        "    self.prelu_atrous1_2 = nn.PReLU(num_parameters=32)\n",
        "    self.bn_atrous1_2 = nn.BatchNorm2d(num_features=32, eps=9.999999747378752e-06, momentum=0.0)\n",
        "    self.conv_atrous1_3 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)\n",
        "    self.prelu_atrous1_3 = nn.PReLU(num_parameters=16)\n",
        "    self.bn_atrous1_3 = nn.BatchNorm2d(num_features=16, eps=9.999999747378752e-06, momentum=0.0)\n",
        "\n",
        "    self.conv_atrous2_1 = nn.Conv2d(in_channels=6, out_channels=32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=1, bias=False)\n",
        "    self.prelu_atrous2_1 = nn.PReLU(num_parameters=32)\n",
        "    self.bn_atrous2_1 = nn.BatchNorm2d(num_features=32, eps=9.999999747378752e-06, momentum=0.0)\n",
        "    self.conv_atrous2_2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=False)\n",
        "    self.prelu_atrous2_2 = nn.PReLU(num_parameters=32)\n",
        "    self.bn_atrous2_2 = nn.BatchNorm2d(num_features=32, eps=9.999999747378752e-06, momentum=0.0)\n",
        "    self.conv_atrous2_3 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)\n",
        "    self.prelu_atrous2_3 = nn.PReLU(num_parameters=16)\n",
        "    self.bn_atrous2_3 = nn.BatchNorm2d(num_features=16, eps=9.999999747378752e-06, momentum=0.0)\n",
        "\n",
        "    self.conv_atrous3_1 = nn.Conv2d(in_channels=6, out_channels=32, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), dilation=(3, 3), groups=1, bias=False)\n",
        "    self.prelu_atrous3_1 = nn.PReLU(num_parameters=32)\n",
        "    self.bn_atrous3_1 = nn.BatchNorm2d(num_features=32, eps=9.999999747378752e-06, momentum=0.0)\n",
        "    self.conv_atrous3_2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=False)\n",
        "    self.prelu_atrous3_2 = nn.PReLU(num_parameters=32)\n",
        "    self.bn_atrous3_2 = nn.BatchNorm2d(num_features=32, eps=9.999999747378752e-06, momentum=0.0)\n",
        "    self.conv_atrous3_3 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)\n",
        "    self.prelu_atrous3_3 = nn.PReLU(num_parameters=16)\n",
        "    self.bn_atrous3_3 = nn.BatchNorm2d(num_features=16, eps=9.999999747378752e-06, momentum=0.0)\n",
        "    \n",
        "    self.conv_s2_down = nn.Conv2d(in_channels=48, out_channels=3, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=True)\n",
        "    self.conv_s2_up = nn.Conv2d(in_channels=3, out_channels=48, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=True)\n",
        "    self.conv_p1_1 = nn.Conv2d(in_channels=48, out_channels=16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=False)\n",
        "    self.prelu_p1_1 = nn.PReLU(num_parameters=16)\n",
        "    self.bn_p1_1 = nn.BatchNorm2d(num_features=16, eps=9.999999747378752e-06, momentum=0.0)\n",
        "    self.pred_step_2 = nn.Conv2d(in_channels=16, out_channels=1, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=True)\n",
        "\n",
        "  def forward(self, dense_out):\n",
        "    concat_input = dense_out[4]\n",
        "    concat_5_16 = dense_out[3]\n",
        "    concat_4_24 = dense_out[2]\n",
        "    concat_3_12 = dense_out[1]\n",
        "    concat_2_6 = dense_out[0]\n",
        "\n",
        "    ################ BLOCK 1 ################      \n",
        "    pool_SE_1_32    = F.avg_pool2d(concat_5_16, kernel_size=(15, 15), stride=(1, 1), padding=(0,), ceil_mode=False, count_include_pad=False)\n",
        "    conv_SE_1_32_1  = self.conv_SE_1_32_1(pool_SE_1_32)\n",
        "    relu_SE_1_32_1  = F.relu(conv_SE_1_32_1)\n",
        "    conv_SE_1_32_2  = self.conv_SE_1_32_2(relu_SE_1_32_1)\n",
        "    sigm_SE_1_32    = torch.sigmoid(conv_SE_1_32_2)\n",
        "    reshape_SE_1_32 = torch.reshape(input = sigm_SE_1_32, shape = (1,1024,1,1))\n",
        "    scale_SE_1_32   = concat_5_16 * reshape_SE_1_32\n",
        "\n",
        "    conv_1_32_1d    = self.conv_1_32_1d(scale_SE_1_32)\n",
        "    #prelu_1_32_1d   = F.prelu(conv_1_32_1d, torch.from_numpy(_weights_dict['prelu_1/32_1d']['weights']))\n",
        "    prelu_1_32_1d   = self.prelu_1_32_1d(conv_1_32_1d)\n",
        "    bn_1_32_1d      = self.bn_1_32_1d(prelu_1_32_1d)\n",
        "\n",
        "    conv_1_32_2d    = self.conv_1_32_2d(bn_1_32_1d)\n",
        "    #prelu_1_32_2d   = F.prelu(conv_1_32_2d, torch.from_numpy(_weights_dict['prelu_1/32_2d']['weights']))\n",
        "    prelu_1_32_2d   = self.prelu_1_32_2d(conv_1_32_2d)\n",
        "    bn_1_32_2d      = self.bn_1_32_2d(prelu_1_32_2d)\n",
        "\n",
        "    conv_1_32_3d    = self.conv_1_32_3d(bn_1_32_2d)\n",
        "    #prelu_1_32_3d   = F.prelu(conv_1_32_3d, torch.from_numpy(_weights_dict['prelu_1/32_3d']['weights']))\n",
        "    prelu_1_32_3d   = self.prelu_1_32_3d(conv_1_32_3d)\n",
        "    bn_1_32_3d      = self.bn_1_32_3d(prelu_1_32_3d)\n",
        "\n",
        "    deconv_1_16d\t  = self.deconv_1_16d(bn_1_32_3d)\n",
        "    #prelu_1_16d     = F.prelu(deconv_1_16d, torch.from_numpy(_weights_dict['prelu_1/16d']['weights']))\n",
        "    prelu_1_16d     = self.prelu_1_16d(deconv_1_16d)\n",
        "    bn_1_16d        = self.bn_1_16d(prelu_1_16d)\n",
        "\n",
        "    pool_SE_1_16    = F.avg_pool2d(concat_4_24, kernel_size=(30, 30), stride=(1, 1), padding=(0,), ceil_mode=False, count_include_pad=False)\n",
        "    conv_SE_1_16_1  = self.conv_SE_1_16_1(pool_SE_1_16)\n",
        "    relu_SE_1_16_1  = F.relu(conv_SE_1_16_1)\n",
        "    conv_SE_1_16_2  = self.conv_SE_1_16_2(relu_SE_1_16_1)\n",
        "    sigm_SE_1_16    = torch.sigmoid(conv_SE_1_16_2)\n",
        "    reshape_SE_1_16 = torch.reshape(input = sigm_SE_1_16, shape = (1,1024,1,1))\n",
        "    scale_SE_1_16   = concat_4_24 * reshape_SE_1_16\n",
        "    conv_SE_1_16    = self.conv_SE_1_16(scale_SE_1_16)\n",
        "    #prelu_SE_1_16    = F.prelu(conv_SE_1_8, torch.from_numpy(_weights_dict['prelu_SE_1/16']['weights']))\n",
        "    prelu_SE_1_16   = self.prelu_SE_1_16(conv_SE_1_16)\n",
        "    bn_SE_1_16      = self.bn_SE_1_16(prelu_SE_1_16)\n",
        "    #########################################\n",
        "\n",
        "    ################ BLOCK 2 ################\n",
        "    concat_1_16d    = torch.cat((bn_1_16d, bn_SE_1_16,), 1)\n",
        "\n",
        "    conv_1_16_1d    = self.conv_1_16_1d(concat_1_16d)\n",
        "    #prelu_1_16_1d   = F.prelu(conv_1_16_1d, torch.from_numpy(_weights_dict['prelu_1/16_1d']['weights']))\n",
        "    prelu_1_16_1d   = self.prelu_1_16_1d(conv_1_16_1d)\n",
        "    bn_1_16_1d      = self.bn_1_16_1d(prelu_1_16_1d)\n",
        "\n",
        "    conv_1_16_2d    = self.conv_1_16_2d(bn_1_16_1d)\n",
        "    #prelu_1_16_2d   = F.prelu(conv_1_16_2d, torch.from_numpy(_weights_dict['prelu_1/16_2d']['weights']))\n",
        "    prelu_1_16_2d   = self.prelu_1_16_2d(conv_1_16_2d)\n",
        "    bn_1_16_2d      = self.bn_1_16_2d(prelu_1_16_2d)\n",
        "\n",
        "    conv_1_16_3d    = self.conv_1_16_3d(bn_1_16_2d)\n",
        "    #prelu_1_16_3d   = F.prelu(conv_1_16_3d, torch.from_numpy(_weights_dict['prelu_1/16_3d']['weights']))\n",
        "    prelu_1_16_3d   = self.prelu_1_16_3d(conv_1_16_3d)\n",
        "    bn_1_16_3d      = self.bn_1_16_3d(prelu_1_16_3d)\n",
        "\n",
        "    deconv_1_8d\t\t= self.deconv_1_8d(bn_1_16_3d)\n",
        "    #prelu_1_8d      = F.prelu(deconv_1_8d, torch.from_numpy(_weights_dict['prelu_1/8d']['weights']))\n",
        "    prelu_1_8d      = self.prelu_1_8d(deconv_1_8d)\n",
        "    bn_1_8d         = self.bn_1_8d(prelu_1_8d)\n",
        "\n",
        "    pool_SE_1_8     = F.avg_pool2d(concat_3_12, kernel_size=(60, 60), stride=(1, 1), padding=(0,), ceil_mode=False, count_include_pad=False)\n",
        "    conv_SE_1_8_1   = self.conv_SE_1_8_1(pool_SE_1_8)        \n",
        "    relu_SE_1_8_1   = F.relu(conv_SE_1_8_1)\n",
        "    conv_SE_1_8_2   = self.conv_SE_1_8_2(relu_SE_1_8_1)\n",
        "    sigm_SE_1_8     = torch.sigmoid(conv_SE_1_8_2)\n",
        "    reshape_SE_1_8  = torch.reshape(input = sigm_SE_1_8, shape = (1,512,1,1))\n",
        "    scale_SE_1_8    = concat_3_12 * reshape_SE_1_8\n",
        "    conv_SE_1_8     = self.conv_SE_1_8(scale_SE_1_8)\n",
        "    #prelu_SE_1_8    = F.prelu(conv_SE_1_8, torch.from_numpy(_weights_dict['prelu_SE_1/8']['weights']))\n",
        "    prelu_SE_1_8    = self.prelu_SE_1_8(conv_SE_1_8)\n",
        "    bn_SE_1_8       = self.bn_SE_1_8(prelu_SE_1_8)\n",
        "    #########################################\n",
        "\n",
        "    ################ BLOCK 3 ################\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
        "    concat_1_8d     = torch.cat((bn_1_8d, bn_SE_1_8,), 1)\n",
        "\n",
        "    conv_1_8_1d     = self.conv_1_8_1d(concat_1_8d)\n",
        "    #prelu_1_8_1d    = F.prelu(conv_1_8_1d, torch.from_numpy(_weights_dict['prelu_1/8_1d']['weights']))\n",
        "    prelu_1_8_1d    = self.prelu_1_8_1d(conv_1_8_1d)\n",
        "    bn_1_8_1d       = self.bn_1_8_1d(prelu_1_8_1d)\n",
        "\n",
        "    conv_1_8_2d     = self.conv_1_8_2d(bn_1_8_1d)\n",
        "    #prelu_1_8_2d    = F.prelu(conv_1_8_2d, torch.from_numpy(_weights_dict['prelu_1/8_2d']['weights']))\n",
        "    prelu_1_8_2d    = self.prelu_1_8_2d(conv_1_8_2d)\n",
        "    bn_1_8_2d       = self.bn_1_8_2d(prelu_1_8_2d)\n",
        "\n",
        "    conv_1_8_3d     = self.conv_1_8_3d(bn_1_8_2d)\n",
        "    #prelu_1_8_3d    = F.prelu(conv_1_8_3d, torch.from_numpy(_weights_dict['prelu_1/8_3d']['weights']))\n",
        "    prelu_1_8_3d    = self.prelu_1_8_3d(conv_1_8_3d)\n",
        "    bn_1_8_3d       = self.bn_1_8_3d(prelu_1_8_3d)\n",
        "\n",
        "    deconv_1_4d\t\t= self.deconv_1_4d(bn_1_8_3d)\n",
        "    #prelu_1_4d      = F.prelu(deconv_1_4d, torch.from_numpy(_weights_dict['prelu_1/4d']['weights']))\n",
        "    prelu_1_4d      = self.prelu_1_4d(deconv_1_4d)\n",
        "    bn_1_4d         = self.bn_1_4d(prelu_1_4d)\n",
        "\n",
        "    pool_SE_1_4     = F.avg_pool2d(concat_2_6, kernel_size=(120, 120), stride=(1, 1), padding=(0,), ceil_mode=False, count_include_pad=False)\n",
        "    conv_SE_1_4_1   = self.conv_SE_1_4_1(pool_SE_1_4)\n",
        "    relu_SE_1_4_1   = F.relu(conv_SE_1_4_1)\n",
        "    conv_SE_1_4_2   = self.conv_SE_1_4_2(relu_SE_1_4_1)\n",
        "    sigm_SE_1_4     = torch.sigmoid(conv_SE_1_4_2)\n",
        "    reshape_SE_1_4  = torch.reshape(input = sigm_SE_1_4, shape = (1,256,1,1))\n",
        "    scale_SE_1_4    = concat_2_6 * reshape_SE_1_4\n",
        "    conv_SE_1_4     = self.conv_SE_1_4(scale_SE_1_4)\n",
        "    #prelu_SE_1_4    = F.prelu(conv_SE_1_4, torch.from_numpy(_weights_dict['prelu_SE_1/4']['weights']))\n",
        "    prelu_SE_1_4    = self.prelu_SE_1_4(conv_SE_1_4)\n",
        "    bn_SE_1_4       = self.bn_SE_1_4(prelu_SE_1_4)\n",
        "    #########################################\n",
        "\n",
        "    ################ BLOCK 4 ################\t\t\t\t\t\t\t\t\t\t\t\t\t\t        \n",
        "    concat_1_4d     = torch.cat((bn_1_4d, bn_SE_1_4,), 1)\n",
        "\n",
        "    conv_1_4_1d     = self.conv_1_4_1d(concat_1_4d)\n",
        "    #prelu_1_4_1d    = F.prelu(conv_1_4_1d, torch.from_numpy(_weights_dict['prelu_1/4_1d']['weights']))\n",
        "    prelu_1_4_1d    = self.prelu_1_4_1d(conv_1_4_1d)\n",
        "    bn_1_4_1d       = self.bn_1_4_1d(prelu_1_4_1d)\n",
        "\n",
        "    conv_1_4_2d     = self.conv_1_4_2d(bn_1_4_1d)\n",
        "    #prelu_1_4_2d    = F.prelu(conv_1_4_2d, torch.from_numpy(_weights_dict['prelu_1/4_2d']['weights']))\n",
        "    prelu_1_4_2d    = self.prelu_1_4_2d(conv_1_4_2d)\n",
        "    bn_1_4_2d       = self.bn_1_4_2d(prelu_1_4_2d)\n",
        "\n",
        "    conv_1_4_3d     = self.conv_1_4_3d(bn_1_4_2d)\n",
        "    #prelu_1_4_3d    = F.prelu(conv_1_4_3d, torch.from_numpy(_weights_dict['prelu_1/4_3d']['weights']))\n",
        "    prelu_1_4_3d    = self.prelu_1_4_3d(conv_1_4_3d)\n",
        "    bn_1_4_3d       = self.bn_1_4_3d(prelu_1_4_3d)\n",
        "    #########################################\n",
        "\n",
        "    ################ PREDICTION AT 1/4 ################\n",
        "    pred_1_4        = self.pred_1_4(bn_1_4_3d)\n",
        "\n",
        "    ################ UNSAMPLE THE PREDICTION FROM 1/4 TO 1/1 ################\n",
        "    pred_step_1 \t= self.pred_step_1(pred_1_4)\n",
        "    sigp_step_1     = torch.sigmoid(pred_step_1)\n",
        "\n",
        "\n",
        "    ################ SECONDARY NETWORK (FINE DECODER) STARTS HERE ################\n",
        "    concat_step_1   = torch.cat((concat_input, sigp_step_1,), 1)\n",
        "\n",
        "    ################ ATROUS POOLING BLOCK 1 ################\n",
        "    conv_atrous1_1  = self.conv_atrous1_1(concat_step_1)\n",
        "    #prelu_atrous1_1 = F.prelu(conv_atrous1_1, torch.from_numpy(_weights_dict['prelu_atrous1_1']['weights']))\n",
        "    prelu_atrous1_1 = self.prelu_atrous1_1(conv_atrous1_1)\n",
        "    bn_atrous1_1    = self.bn_atrous1_1(prelu_atrous1_1)\n",
        "\n",
        "    conv_atrous1_2  = self.conv_atrous1_2(bn_atrous1_1)\n",
        "    #prelu_atrous1_2 = F.prelu(conv_atrous1_2, torch.from_numpy(_weights_dict['prelu_atrous1_2']['weights']))\n",
        "    prelu_atrous1_2 = self.prelu_atrous1_2(conv_atrous1_2)\n",
        "    bn_atrous1_2    = self.bn_atrous1_2(prelu_atrous1_2)\n",
        "\n",
        "    conv_atrous1_3  = self.conv_atrous1_3(bn_atrous1_2)\n",
        "    #prelu_atrous1_3 = F.prelu(conv_atrous1_3, torch.from_numpy(_weights_dict['prelu_atrous1_3']['weights']))\n",
        "    prelu_atrous1_3 = self.prelu_atrous1_3(conv_atrous1_3)\n",
        "    bn_atrous1_3    = self.bn_atrous1_3(prelu_atrous1_3)\n",
        "    ########################################################\n",
        "\n",
        "    ################ ATROUS POOLING BLOCK 2 ################\n",
        "    conv_atrous2_1  = self.conv_atrous2_1(concat_step_1)\n",
        "    #prelu_atrous2_1 = F.prelu(conv_atrous2_1, torch.from_numpy(_weights_dict['prelu_atrous2_1']['weights']))\n",
        "    prelu_atrous2_1 = self.prelu_atrous2_1(conv_atrous2_1)\n",
        "    bn_atrous2_1    = self.bn_atrous2_1(prelu_atrous2_1)\n",
        "\n",
        "    conv_atrous2_2  = self.conv_atrous2_2(bn_atrous2_1)\n",
        "    #prelu_atrous2_2 = F.prelu(conv_atrous2_2, torch.from_numpy(_weights_dict['prelu_atrous2_2']['weights']))\n",
        "    prelu_atrous2_2 = self.prelu_atrous2_2(conv_atrous2_2)\n",
        "    bn_atrous2_2    = self.bn_atrous2_2(prelu_atrous2_2)\n",
        "\n",
        "    conv_atrous2_3  = self.conv_atrous2_3(bn_atrous2_2)\n",
        "    #prelu_atrous2_3 = F.prelu(conv_atrous2_3, torch.from_numpy(_weights_dict['prelu_atrous2_3']['weights']))\n",
        "    prelu_atrous2_3 = self.prelu_atrous2_3(conv_atrous2_3)\n",
        "    bn_atrous2_3    = self.bn_atrous2_3(prelu_atrous2_3)\n",
        "    ########################################################\n",
        "\n",
        "    ################ ATROUS POOLING BLOCK 3 ################\n",
        "    conv_atrous3_1  = self.conv_atrous3_1(concat_step_1)\n",
        "    #prelu_atrous3_1 = F.prelu(conv_atrous3_1, torch.from_numpy(_weights_dict['prelu_atrous3_1']['weights']))\n",
        "    prelu_atrous3_1 = self.prelu_atrous3_1(conv_atrous3_1)\n",
        "    bn_atrous3_1    = self.bn_atrous3_1(prelu_atrous3_1)\n",
        "\n",
        "    conv_atrous3_2  = self.conv_atrous3_2(bn_atrous3_1)\n",
        "    #prelu_atrous3_2 = F.prelu(conv_atrous3_2, torch.from_numpy(_weights_dict['prelu_atrous3_2']['weights']))\n",
        "    prelu_atrous3_2 = self.prelu_atrous3_2(conv_atrous3_2)\n",
        "    bn_atrous3_2    = self.bn_atrous3_2(prelu_atrous3_2)\n",
        "\n",
        "    conv_atrous3_3  = self.conv_atrous3_3(bn_atrous3_2)\n",
        "    #prelu_atrous3_3 = F.prelu(conv_atrous3_3, torch.from_numpy(_weights_dict['prelu_atrous3_3']['weights']))\n",
        "    prelu_atrous3_3 = self.prelu_atrous3_3(conv_atrous3_3)\n",
        "    bn_atrous3_3    = self.bn_atrous3_3(prelu_atrous3_3)\n",
        "    ########################################################\n",
        "\n",
        "    ################ CONCAT + SQUEEZ & EXCITATION ################\n",
        "    concat_step_2   = torch.cat((bn_atrous1_3, bn_atrous2_3, bn_atrous3_3,), 1)\n",
        "    gpool_s2        = F.avg_pool2d(concat_step_2, kernel_size=(480, 480), stride=(1, 1), padding=(0,), ceil_mode=False, count_include_pad=False)\n",
        "    conv_s2_down    = self.conv_s2_down(gpool_s2)\n",
        "    relu_s2_down    = F.relu(conv_s2_down)\n",
        "    conv_s2_up      = self.conv_s2_up(relu_s2_down)\n",
        "    sig_s2_up       = torch.sigmoid(conv_s2_up)\n",
        "    resh_s2         = torch.reshape(input = sig_s2_up, shape = (1,48,1,1))\n",
        "    scale_s2        = concat_step_2 * resh_s2\n",
        "    ##############################################################\n",
        "\n",
        "    ################ PREDICTION ################\n",
        "    conv_p1_1       = self.conv_p1_1(scale_s2)\n",
        "    #prelu_p1_1      = F.prelu(conv_p1_1, torch.from_numpy(_weights_dict['prelu_p1_1']['weights']))\n",
        "    prelu_p1_1      = self.prelu_p1_1(conv_p1_1)\n",
        "    bn_p1_1         = self.bn_p1_1(prelu_p1_1)\n",
        "    pred_step_2     = self.pred_step_2(bn_p1_1)\n",
        "    ############################################\n",
        "\n",
        "    ################ PREDICTION ################\n",
        "    sig_pred        = torch.sigmoid(pred_step_2)\n",
        "    ############################################\n",
        "    return sig_pred\n",
        "\n",
        "    ################ SECONDARY NETWORK (FINE DECORDER) ENDS HERE ################"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsAdxqKGa96m"
      },
      "source": [
        "# Initialise encoder object\n",
        "encoder = densenet121(include_ss=True).double()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkdTieZAIigc"
      },
      "source": [
        "# Initialise decoder\n",
        "decoder = Decoder().double()"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqq_jIhVIeGi"
      },
      "source": [
        "# Test Encoder\n",
        "img, target = sbd.__getitem__(4)\n",
        "pos_clicks, neg_clicks = gen_clicks(target)\n",
        "iact_map, click_location, click_type = convert_clicks(pos_clicks, neg_clicks)\n",
        "input = torch.cat((img, iact_map), 0)\n",
        "input = input.unsqueeze(0).double().to(device)\n",
        "target = target.to(device)\n",
        "encoder_out, dense_out, ss_out = encoder(input)\n",
        "dense_out.append(input)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VunvleOMzAUh",
        "outputId": "36113e6a-ba44-4eca-db4e-91b514d6a778"
      },
      "source": [
        "for out in ss_out:\n",
        "  print(out.shape)\n",
        "print(encoder_out.shape)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1000])\n",
            "torch.Size([1, 1000])\n",
            "torch.Size([1, 1000])\n",
            "torch.Size([1, 1000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIQCkEUDIfLm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49c60c8e-cdf7-485f-9efa-b64569f28406"
      },
      "source": [
        "# Test Decoder\n",
        "decoder_out = decoder(dense_out)\n",
        "decoder_out = torch.where(decoder_out > 0.5, 1.0, 0.0)\n",
        "print(decoder_out.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1, 480, 480])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qnqvDHfcshI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "outputId": "e53a0c9d-6947-42cc-87bb-836244ea58ae"
      },
      "source": [
        "# Visualize Decoder output\n",
        "plt.imshow(transforms.ToPILImage()(decoder_out[0]))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f02b24435d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjylSdUUacPk"
      },
      "source": [
        "# Calculate IOU(intersection Over Union) between output and target\n",
        "def iou_metric(output, target):\n",
        "  output = output.squeeze(1)\n",
        "  \n",
        "  intersection = (torch.logical_and(output, target)).float().sum((1, 2))  # Will be zero if Truth=0 or Prediction=0\n",
        "  union = (torch.logical_or(output, target)).float().sum((1, 2))         # Will be zero if both are 0\n",
        "  \n",
        "  iou = (intersection + 1e-6) / (union + 1e-6)  # We smooth our division to avoid 0/0\n",
        "  print(intersection, union)\n",
        "  \n",
        "  #thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() / 10  # This is equal to comparing with thresolds\n",
        "  \n",
        "  return iou"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1d_PHwZ4Y_m",
        "outputId": "3bdd4018-710e-4d4f-eda8-c03ddba14294"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "print(torch.cuda.device_count())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5D6yjVJb6tY"
      },
      "source": [
        "encoder = densenet121().double().to(device)\n",
        "decoder = Decoder().double().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKmXxXCbWdmg"
      },
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD([{'params': encoder.parameters(), 'lr': 1e-9}, {'params': decoder.parameters(), 'lr': 1e-7}], momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD67CB_3xtXj"
      },
      "source": [
        "def train_model(encoder, decoder, dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i in range(dataset.__len__()):\n",
        "      img, target = dataset.__getitem__(i)\n",
        "      pos_clicks, neg_clicks = gen_clicks(target)\n",
        "      iact_map, click_location, click_type = convert_clicks(pos_clicks, neg_clicks)\n",
        "      input = torch.cat((img, iact_map), 0)\n",
        "      input = input.unsqueeze(0).double().to(device)\n",
        "      target = target.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      encoder_out, dense_out, ss_out = encoder(input)\n",
        "      dense_out.append(input)\n",
        "      decoder_out = decoder(dense_out)   \n",
        "\n",
        "      loss = criterion(decoder_out, target.unsqueeze(0).double())\n",
        "      loss.backward()\n",
        "      optimizer.step() \n",
        "\n",
        "      running_loss += loss.item()\n",
        "      if i % 500 == 0:\n",
        "        print(\"{} loss: {:.4f}\".format(i, running_loss/(i+1)))\n",
        "    \n",
        "    print(\"{}, loss: {:.4f}\".format(epoch, running_loss/dataset.__len__()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7AYuSbyi96o",
        "outputId": "8c248d91-26e8-47e1-d23c-dd6dd302e451"
      },
      "source": [
        "train_model(encoder, decoder, sbd, 30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 loss: 0.7428\n",
            "500 loss: 0.6298\n",
            "1000 loss: 0.6278\n",
            "1500 loss: 0.6269\n",
            "2000 loss: 0.6255\n",
            "2500 loss: 0.6244\n",
            "3000 loss: 0.6231\n",
            "3500 loss: 0.6233\n",
            "4000 loss: 0.6236\n",
            "4500 loss: 0.6236\n",
            "5000 loss: 0.6240\n",
            "5500 loss: 0.6238\n",
            "6000 loss: 0.6235\n",
            "6500 loss: 0.6229\n",
            "7000 loss: 0.6226\n",
            "7500 loss: 0.6219\n",
            "0, loss: 0.6209\n",
            "0 loss: 0.6539\n",
            "500 loss: 0.6102\n",
            "1000 loss: 0.6066\n",
            "1500 loss: 0.6059\n",
            "2000 loss: 0.6050\n",
            "2500 loss: 0.6045\n",
            "3000 loss: 0.6032\n",
            "3500 loss: 0.6032\n",
            "4000 loss: 0.6036\n",
            "4500 loss: 0.6037\n",
            "5000 loss: 0.6044\n",
            "5500 loss: 0.6043\n",
            "6000 loss: 0.6040\n",
            "6500 loss: 0.6035\n",
            "7000 loss: 0.6035\n",
            "7500 loss: 0.6030\n",
            "1, loss: 0.6023\n",
            "0 loss: 0.7256\n",
            "500 loss: 0.5946\n",
            "1000 loss: 0.5924\n",
            "1500 loss: 0.5917\n",
            "2000 loss: 0.5914\n",
            "2500 loss: 0.5912\n",
            "3000 loss: 0.5904\n",
            "3500 loss: 0.5900\n",
            "4000 loss: 0.5903\n",
            "4500 loss: 0.5904\n",
            "5000 loss: 0.5908\n",
            "5500 loss: 0.5908\n",
            "6000 loss: 0.5905\n",
            "6500 loss: 0.5902\n",
            "7000 loss: 0.5904\n",
            "7500 loss: 0.5902\n",
            "2, loss: 0.5898\n",
            "0 loss: 0.6751\n",
            "500 loss: 0.5808\n",
            "1000 loss: 0.5785\n",
            "1500 loss: 0.5774\n",
            "2000 loss: 0.5785\n",
            "2500 loss: 0.5786\n",
            "3000 loss: 0.5788\n",
            "3500 loss: 0.5784\n",
            "4000 loss: 0.5793\n",
            "4500 loss: 0.5797\n",
            "5000 loss: 0.5810\n",
            "5500 loss: 0.5810\n",
            "6000 loss: 0.5806\n",
            "6500 loss: 0.5802\n",
            "7000 loss: 0.5805\n",
            "7500 loss: 0.5806\n",
            "3, loss: 0.5801\n",
            "0 loss: 0.6479\n",
            "500 loss: 0.5736\n",
            "1000 loss: 0.5722\n",
            "1500 loss: 0.5715\n",
            "2000 loss: 0.5718\n",
            "2500 loss: 0.5722\n",
            "3000 loss: 0.5721\n",
            "3500 loss: 0.5719\n",
            "4000 loss: 0.5720\n",
            "4500 loss: 0.5724\n",
            "5000 loss: 0.5736\n",
            "5500 loss: 0.5738\n",
            "6000 loss: 0.5738\n",
            "6500 loss: 0.5737\n",
            "7000 loss: 0.5740\n",
            "7500 loss: 0.5739\n",
            "4, loss: 0.5737\n",
            "0 loss: 0.6189\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-573e4d3268ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msbd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-c2014c3cc5cb>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(encoder, decoder, dataset, epochs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7V0UlFsIH2V"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "     transforms.Resize((480,480))])\n",
        "\n",
        "cifar10 = CIFAR10(root=\".\", train=True, transform=transform, download=True)\n",
        "trainloader = torch.utils.data.DataLoader(cifar10, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByK6vJClIKCw"
      },
      "source": [
        "encoder = densenet121(num_classes=10).double().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(encoder.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWrffp_tINDY"
      },
      "source": [
        "# Trains the model\n",
        "def pretrain_model(encoder, dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "      img, target = data\n",
        "      target = target.to(device)\n",
        "      #pos_clicks, neg_clicks = gen_clicks(target)\n",
        "      #iact_map, click_location, click_type = convert_clicks(pos_clicks, neg_clicks)\n",
        "      iact_map = torch.ones((img.shape[0],2,480,480))\n",
        "      input = torch.cat((img, iact_map), 1)\n",
        "      input = input.double().to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      encoder_out, dense_out, ss_out = encoder(input)\n",
        "      dense_out.append(input) \n",
        "\n",
        "      #loss = criterion(encoder_out, target.unsqueeze(0).double())\n",
        "      loss = criterion(encoder_out, target)\n",
        "      loss.backward()\n",
        "      optimizer.step() \n",
        "\n",
        "      running_loss += loss.item()\n",
        "      if i % 100 == 0:\n",
        "        print(\"{} loss: {:.4f}\".format(i, running_loss/(i+1)))\n",
        "    \n",
        "    print(\"{}, loss: {:.4f}\".format(epoch, running_loss/dataset.__len__()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8J2L_bHTIOmv"
      },
      "source": [
        "pretrain_model(encoder, cifar10, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkfBGWhLPHT2"
      },
      "source": [
        "# Save model\n",
        "torch.save(encoder.state_dict(), \"./encoder_dict.pth\")\n",
        "torch.save(decoder.state_dict(), \"./decoder_dict.pth\")\n",
        "torch.save(optimizer.state_dict(), \"./optimizer_dict.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7xYxv1ePH4Y"
      },
      "source": [
        "# Load Model\n",
        "encoder.load_state_dict(torch.load(\"./encoder_dict.pth\"))\n",
        "decoder.load_state_dict(torch.load(\"./decoder_dict.pth\"))\n",
        "optimizer.load_state_dict(torch.load(\"./optimizer_dict.pth\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2NgqAOgPLBI"
      },
      "source": [
        "# Load Model\n",
        "encoder.load_state_dict(torch.load(\"./encoder_dict.pth\", map_location=torch.device('cpu')), strict=False)\n",
        "decoder.load_state_dict(torch.load(\"./decoder_dict.pth\", map_location=torch.device('cpu')))\n",
        "optimizer.load_state_dict(torch.load(\"./optimizer_dict.pth\", map_location=torch.device('cpu')))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}