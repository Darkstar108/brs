{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BRS++.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eb663c50af91455a999e51bdbd832248": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d99b07e71f1b41409342b4e1c962889f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_595529fb69f340c18f1ec2b320360791",
              "IPY_MODEL_8bb6eb4228cc4f9b962524bc98c5a1df"
            ]
          }
        },
        "d99b07e71f1b41409342b4e1c962889f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "595529fb69f340c18f1ec2b320360791": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e2dafbe158154d38a893e48e30e9c9c7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ab46d55a2ba24d568eaaee90cf0fa74a"
          }
        },
        "8bb6eb4228cc4f9b962524bc98c5a1df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5001711a32aa4fa89ae07799aab8eed9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1419542528/? [12:40&lt;00:00, 3478363.09it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d99f08d39f5940428b37fa8f0c7af66f"
          }
        },
        "e2dafbe158154d38a893e48e30e9c9c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ab46d55a2ba24d568eaaee90cf0fa74a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5001711a32aa4fa89ae07799aab8eed9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d99f08d39f5940428b37fa8f0c7af66f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1bb80273f61240c093873af6df60dc43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_738fd6fd91cf45e09ff0f62137e7788c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6a3daacf15fe463b997e998b2bc38a86",
              "IPY_MODEL_aef204d5a109495ca9472194832b4d29"
            ]
          }
        },
        "738fd6fd91cf45e09ff0f62137e7788c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6a3daacf15fe463b997e998b2bc38a86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_95eb429207d24598b0420d1ef7b11568",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3bf3513fbce942799d94d8eb386518b4"
          }
        },
        "aef204d5a109495ca9472194832b4d29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_89692a52278f40b6be1bd518b924ed10",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 73728/? [00:04&lt;00:00, 18267.88it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fb5a9fe5b923427392bf6f53d2fbdf17"
          }
        },
        "95eb429207d24598b0420d1ef7b11568": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3bf3513fbce942799d94d8eb386518b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "89692a52278f40b6be1bd518b924ed10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fb5a9fe5b923427392bf6f53d2fbdf17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4cTLmB-_7kt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf68c5df-a3e9-433f-b073-8e431867f6bd"
      },
      "source": [
        "%pip install scikit-learn-extra"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn-extra in /usr/local/lib/python3.6/dist-packages (0.1.0b2)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn-extra) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn-extra) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn-extra) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0->scikit-learn-extra) (1.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz67dvcyu6pA"
      },
      "source": [
        "# Imports\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "from sklearn_extra.cluster import KMedoids\r\n",
        "from scipy import ndimage\r\n",
        "import tensorflow as tf\r\n",
        "import os\r\n",
        "import os.path\r\n",
        "import random\r\n",
        "import shutil\r\n",
        "import hashlib\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.utils.checkpoint as cp\r\n",
        "import torch.utils.data as data\r\n",
        "from torch.utils.model_zoo import tqdm\r\n",
        "import tarfile\r\n",
        "import zipfile\r\n",
        "from torch import Tensor\r\n",
        "from torchvision import datasets\r\n",
        "from torchvision.datasets import SBDataset\r\n",
        "from torchvision import transforms\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from PIL import Image\r\n",
        "from tqdm.auto import tqdm\r\n",
        "from collections import OrderedDict\r\n",
        "from typing import Any, List, Tuple, Optional, Callable, Dict, TypeVar, Iterable\r\n",
        "from vision import *\r\n",
        "from utils import *\r\n",
        "\r\n",
        "random.seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNjDh3pGt1hI"
      },
      "source": [
        "**Steps to do:**\r\n",
        "\r\n",
        "*   Preprocessing Pipeline\r\n",
        "  *   Part of Dataset init()\r\n",
        "     *   Ignore images with dimension lower than filter_size x filter_size\r\n",
        "     *   For images with multiple labels, pick one and let the others be part of the background\r\n",
        "  *   Part of Transform or Function\r\n",
        "     *   Take a filter_size x filter_size crop by resampling crops until the complete object instance is inside the crop\r\n",
        "     *   Upscale image to 480x480\r\n",
        "     *   Use k-mediods to generate random number of foreground & background clicks\r\n",
        "     *   Generate interaction maps\r\n",
        "*   Model\r\n",
        "  *   DenseNet-121 Encoder\r\n",
        "  *   Decoder from BRS paper\r\n",
        "  *   Add ASPP module between Encoder & Decoder\r\n",
        "  *   Add Semantic Supervision block to Encoder during pretraining\r\n",
        "  *   Add LIP for pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pQQOaT55kfY"
      },
      "source": [
        "# Function to convert PIL images to Tensors. We can pass this as a transform to the Dataset\r\n",
        "def PIL_to_tensor(img, target):\r\n",
        "  return transforms.ToTensor()(img), transforms.ToTensor()(target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FDsEkPyt4hK"
      },
      "source": [
        "def download_extract(url: str, root: str, filename: str, md5: str) -> None:\r\n",
        "    download_url(url, root, filename, md5)\r\n",
        "    with tarfile.open(os.path.join(root, filename), \"r\") as tar:\r\n",
        "        tar.extractall(path=root)\r\n",
        "\r\n",
        "# Remember to upload vision.py and utils.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpdKdu90t7XL"
      },
      "source": [
        "class CustomSBDataset(VisionDataset):\r\n",
        "    \"\"\"`Semantic Boundaries Dataset <http://home.bharathh.info/pubs/codes/SBD/download.html>`_\r\n",
        "\r\n",
        "    Args:\r\n",
        "        root (string): Root directory of the Semantic Boundaries Dataset\r\n",
        "        image_set (string, optional): Select the image_set to use, ``train``, ``val`` or ``train_noval``.\r\n",
        "            Image set ``train_noval`` excludes VOC 2012 val images.\r\n",
        "        mode (string, optional): Select target type. Possible values 'boundaries' or 'segmentation'.\r\n",
        "            In case of 'boundaries', the target is an array of shape `[num_classes, H, W]`,\r\n",
        "            where `num_classes=20`.\r\n",
        "        download (bool, optional): If true, downloads the dataset from the internet and\r\n",
        "            puts it in root directory. If dataset is already downloaded, it is not\r\n",
        "            downloaded again.\r\n",
        "        transforms (callable, optional): A function/transform that takes input sample and its target as entry\r\n",
        "            and returns a transformed version. Input sample is PIL image and target is a numpy array\r\n",
        "            if `mode='boundaries'` or PIL image if `mode='segmentation'`.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    url = \"http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/semantic_contours/benchmark.tgz\"\r\n",
        "    md5 = \"82b4d87ceb2ed10f6038a1cba92111cb\"\r\n",
        "    filename = \"benchmark.tgz\"\r\n",
        "\r\n",
        "    voc_train_url = \"http://home.bharathh.info/pubs/codes/SBD/train_noval.txt\"\r\n",
        "    voc_split_filename = \"train_noval.txt\"\r\n",
        "    voc_split_md5 = \"79bff800c5f0b1ec6b21080a3c066722\"\r\n",
        "\r\n",
        "    def __init__(\r\n",
        "            self,\r\n",
        "            root: str,\r\n",
        "            image_set: str = \"train\",\r\n",
        "            mode: str = \"segmentation\",\r\n",
        "            download: bool = False,\r\n",
        "            img_filter_size = 200,\r\n",
        "            transforms: Optional[Callable] = None,\r\n",
        "    ) -> None:\r\n",
        "\r\n",
        "        try:\r\n",
        "            from scipy.io import loadmat\r\n",
        "            self._loadmat = loadmat\r\n",
        "        except ImportError:\r\n",
        "            raise RuntimeError(\"Scipy is not found. This dataset needs to have scipy installed: \"\r\n",
        "                               \"pip install scipy\")\r\n",
        "\r\n",
        "        super(CustomSBDataset, self).__init__(root, transforms)\r\n",
        "        self.image_set = verify_str_arg(image_set, \"image_set\",\r\n",
        "                                        (\"train\", \"val\", \"train_noval\"))\r\n",
        "        self.mode = verify_str_arg(mode, \"mode\", (\"segmentation\", \"boundaries\"))\r\n",
        "        self.num_classes = 20\r\n",
        "        self.img_filter_size = img_filter_size\r\n",
        "\r\n",
        "        sbd_root = self.root\r\n",
        "        image_dir = os.path.join(sbd_root, 'img')\r\n",
        "        mask_dir = os.path.join(sbd_root, 'inst')\r\n",
        "\r\n",
        "        if download:\r\n",
        "            self.download_dataset(sbd_root)\r\n",
        "\r\n",
        "        if not os.path.isdir(sbd_root):\r\n",
        "            raise RuntimeError('Dataset not found or corrupted.' +\r\n",
        "                               ' You can use download=True to download it')\r\n",
        "            \r\n",
        "        self.load_filenames(sbd_root, image_dir, mask_dir, image_set)\r\n",
        "\r\n",
        "        self._get_target = self._get_segmentation_target \\\r\n",
        "            if self.mode == \"segmentation\" else self._get_boundaries_target\r\n",
        "\r\n",
        "        #self.filter_images()\r\n",
        "\r\n",
        "    def _get_segmentation_target(self, filepath: str) -> Image.Image:\r\n",
        "        mat = self._loadmat(filepath)\r\n",
        "        return Image.fromarray(mat['GTinst'][0]['Segmentation'][0])\r\n",
        "\r\n",
        "    def _get_boundaries_target(self, filepath: str) -> np.ndarray:\r\n",
        "        mat = self._loadmat(filepath)\r\n",
        "        return np.concatenate([np.expand_dims(mat['GTinst'][0]['Boundaries'][0][i][0].toarray(), axis=0)\r\n",
        "                               for i in range(self.num_classes)], axis=0)\r\n",
        "\r\n",
        "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\r\n",
        "        img = Image.open(self.images[index]).convert('RGB')\r\n",
        "        target = self._get_target(self.masks[index])\r\n",
        "\r\n",
        "        if self.transforms is not None:\r\n",
        "            img, target = self.transforms(img, target)\r\n",
        "\r\n",
        "        img = transforms.ToTensor()(img)\r\n",
        "        target = (transforms.ToTensor()(target)*255)\r\n",
        "        target = self.naive_ignore_multiple_object_instances(target)\r\n",
        "        img, target = self.center_crop(img, target)\r\n",
        "        img = transforms.Resize((480, 480))(img)\r\n",
        "        target = transforms.Resize((480, 480))(target)\r\n",
        "\r\n",
        "        return img, target\r\n",
        "\r\n",
        "    def __len__(self) -> int:\r\n",
        "        return len(self.images)\r\n",
        "\r\n",
        "    def extra_repr(self) -> str:\r\n",
        "        lines = [\"Image set: {image_set}\", \"Mode: {mode}\"]\r\n",
        "        return '\\n'.join(lines).format(**self.__dict__)\r\n",
        "\r\n",
        "    def download_dataset(self, sbd_root) -> None:\r\n",
        "        download_extract(self.url, self.root, self.filename, self.md5)\r\n",
        "        extracted_ds_root = os.path.join(self.root, \"benchmark_RELEASE\", \"dataset\")\r\n",
        "\r\n",
        "        for f in [\"cls\", \"img\", \"inst\", \"train.txt\", \"val.txt\"]:\r\n",
        "            old_path = os.path.join(extracted_ds_root, f)\r\n",
        "            shutil.move(old_path, sbd_root)\r\n",
        "        download_url(self.voc_train_url, sbd_root, self.voc_split_filename,\r\n",
        "                      self.voc_split_md5)\r\n",
        "        \r\n",
        "    def load_filenames(self, sbd_root, image_dir, mask_dir, image_set) -> None:\r\n",
        "        split_f = os.path.join(sbd_root, image_set.rstrip('\\n') + '.txt')\r\n",
        "\r\n",
        "        with open(os.path.join(split_f), \"r\") as fh:\r\n",
        "            file_names = [x.strip() for x in fh.readlines()]\r\n",
        "\r\n",
        "        self.images = [os.path.join(image_dir, x + \".jpg\") for x in file_names]\r\n",
        "        self.masks = [os.path.join(mask_dir, x + \".mat\") for x in file_names]\r\n",
        "        assert (len(self.images) == len(self.masks))\r\n",
        "        \r\n",
        "    # Remove images with both height and width lower than img_filter_size\r\n",
        "    def filter_images(self):\r\n",
        "        img_to_remove = set()\r\n",
        "        for i in range(len(self.images)):\r\n",
        "            img = Image.open(self.images[i])\r\n",
        "            width, height = img.size\r\n",
        "            if width < self.img_filter_size or height < self.img_filter_size:\r\n",
        "                img_to_remove.add(i)\r\n",
        "\r\n",
        "        self.images = [img for index, img in enumerate(self.images) if index not in img_to_remove]\r\n",
        "        self.masks = [mask for index, mask in enumerate(self.masks) if index not in img_to_remove]\r\n",
        "\r\n",
        "    # For targets that have multiple object instances, pick label 1 as foreground and let the others be part of the background\r\n",
        "    def naive_ignore_multiple_object_instances(self, target):\r\n",
        "        cond = torch.eq(target, torch.ones_like(target))\r\n",
        "        target = torch.where(cond, target, torch.zeros_like(target))\r\n",
        "        return target\r\n",
        "\r\n",
        "    # Apply CenterCrop to image to make it a square based on its smaller dimension\r\n",
        "    def center_crop(self, image, target):\r\n",
        "      min_len = min(image.shape[1], image.shape[2])\r\n",
        "      image = transforms.CenterCrop(min_len)(image)\r\n",
        "      target = transforms.CenterCrop(min_len)(target)\r\n",
        "      return image, target\r\n",
        "\r\n",
        "    # Apply Random Crop on Image & Target while keeping most of the object instance inside the crop\r\n",
        "    def random_crop(self, image, target):\r\n",
        "      left = -1\r\n",
        "      right = -1\r\n",
        "      top = -1\r\n",
        "      bottom = -1\r\n",
        "\r\n",
        "      # Find the extreme points of the object instance in the target\r\n",
        "      res = torch.nonzero(target[0])\r\n",
        "      values, indices = torch.min(res, 0)\r\n",
        "      top = values[0].item()\r\n",
        "      left = values[1].item()\r\n",
        "      values, indices = torch.max(res, 0)\r\n",
        "      bottom = values[0].item()\r\n",
        "      right = values[1].item()\r\n",
        "\r\n",
        "      # Calculate range to sample top left crop point from\r\n",
        "      if right - left >= self.img_filter_size:\r\n",
        "        x_min = (right - left)//2\r\n",
        "        x_max = (right - left)//2\r\n",
        "      else:\r\n",
        "        x_min = min(0, right - self.img_filter_size)\r\n",
        "        x_max = left\r\n",
        "\r\n",
        "      if bottom - top >= self.img_filter_size:\r\n",
        "        y_min = (bottom - top)//2\r\n",
        "        y_max = (bottom - top)//2\r\n",
        "      else:\r\n",
        "        y_min = min(0, bottom - self.img_filter_size)\r\n",
        "        y_max = top\r\n",
        "\r\n",
        "      # Sample top left crop point\r\n",
        "      x = random.randint(x_min, x_max)\r\n",
        "      y = random.randint(y_min, y_max)\r\n",
        "\r\n",
        "      # Apply same random crop to both image and target\r\n",
        "      image = transforms.functional.crop(image, y, x, self.img_filter_size, self.img_filter_size)\r\n",
        "      target = transforms.functional.crop(target, y, x, self.img_filter_size, self.img_filter_size)\r\n",
        "\r\n",
        "      return image, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y6em226-vfx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "eb663c50af91455a999e51bdbd832248",
            "d99b07e71f1b41409342b4e1c962889f",
            "595529fb69f340c18f1ec2b320360791",
            "8bb6eb4228cc4f9b962524bc98c5a1df",
            "e2dafbe158154d38a893e48e30e9c9c7",
            "ab46d55a2ba24d568eaaee90cf0fa74a",
            "5001711a32aa4fa89ae07799aab8eed9",
            "d99f08d39f5940428b37fa8f0c7af66f",
            "1bb80273f61240c093873af6df60dc43",
            "738fd6fd91cf45e09ff0f62137e7788c",
            "6a3daacf15fe463b997e998b2bc38a86",
            "aef204d5a109495ca9472194832b4d29",
            "95eb429207d24598b0420d1ef7b11568",
            "3bf3513fbce942799d94d8eb386518b4",
            "89692a52278f40b6be1bd518b924ed10",
            "fb5a9fe5b923427392bf6f53d2fbdf17"
          ]
        },
        "outputId": "1183f4a0-4f28-44ce-968d-1f8c56b489e2"
      },
      "source": [
        "# Download the training set\r\n",
        "sbd = CustomSBDataset(root=\".\", image_set=\"train\", mode=\"segmentation\", download=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/semantic_contours/benchmark.tgz to ./benchmark.tgz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb663c50af91455a999e51bdbd832248",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://home.bharathh.info/pubs/codes/SBD/train_noval.txt to ./train_noval.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1bb80273f61240c093873af6df60dc43",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W19rDu1Q5-o1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "e2e35def-4ce0-4ef2-9c2c-d0d8cd6ec223"
      },
      "source": [
        "# Download the validation Dataset\r\n",
        "sbd_val = CustomSBDataset(root=\".\", image_set=\"val\", mode=\"segmentation\", download=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: ./benchmark.tgz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-3471529688d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Download the validation Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msbd_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomSBDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"segmentation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-28-bcc94acd4702>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, image_set, mode, download, img_filter_size, transforms)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msbd_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msbd_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-bcc94acd4702>\u001b[0m in \u001b[0;36mdownload_dataset\u001b[0;34m(self, sbd_root)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"cls\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"img\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inst\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"val.txt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mold_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextracted_ds_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msbd_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         download_url(self.voc_train_url, sbd_root, self.voc_split_filename,\n\u001b[1;32m    108\u001b[0m                       self.voc_split_md5)\n",
            "\u001b[0;32m/usr/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mreal_dst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Destination path '%s' already exists\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: Destination path './cls' already exists"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kaV9X_w-3GK"
      },
      "source": [
        "# Check img and target shape\r\n",
        "img, target = sbd.__getitem__(0)\r\n",
        "print(img.shape, target.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_qE74Q06PNt"
      },
      "source": [
        "# Code to display image and segmentation mask. Set images to be num of image, segmentation masks to display\r\n",
        "images = 10\r\n",
        "fig,axes = plt.subplots(nrows = images, ncols = 2, figsize=(50,50))\r\n",
        "\r\n",
        "for i in range(images):\r\n",
        "    img, target = sbd.__getitem__(i)\r\n",
        "    axes[i,0].imshow(img)\r\n",
        "    axes[i,1].imshow(target)\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yz9qkVk7CZog"
      },
      "source": [
        "# Sample user clicks using target\r\n",
        "def gen_clicks(target):\r\n",
        "  num_pos = random.randint(1, 10)\r\n",
        "  num_neg = random.randint(0, 10)\r\n",
        "\r\n",
        "  pos_clicks = gen_pos_clicks(target, num_pos)\r\n",
        "  neg_clicks = gen_neg_clicks(target, num_neg)\r\n",
        "\r\n",
        "  return pos_clicks, neg_clicks\r\n",
        "\r\n",
        "# Generate n +ve clicks by randomly sampling points in the foreground\r\n",
        "def gen_pos_clicks(target, n):\r\n",
        "  pos_clicks = torch.zeros((n, 2))\r\n",
        "  dstep = 5\r\n",
        "  dmargin = 5\r\n",
        "\r\n",
        "  distances = ndimage.distance_transform_edt(target[0].numpy())\r\n",
        "  distances = np.where(distances < dmargin, 0, distances)\r\n",
        "  points = torch.nonzero(torch.from_numpy(distances))\r\n",
        "  print(len(points))\r\n",
        "\r\n",
        "  for i in range(n):\r\n",
        "    resample = 1\r\n",
        "    while resample > 0:\r\n",
        "      index = random.randint(0, len(points))\r\n",
        "      point = points[index]\r\n",
        "      if len(pos_clicks) == 0:\r\n",
        "        pos_clicks[i][0] = point[0]\r\n",
        "        pos_clicks[i][1] = point[1]\r\n",
        "        resample = 0\r\n",
        "      else:\r\n",
        "        min_dist = dstep\r\n",
        "        for pos in pos_clicks:\r\n",
        "          dist = torch.dist(point.type(torch.FloatTensor), pos.type(torch.FloatTensor), 2)\r\n",
        "          if dist < min_dist:\r\n",
        "            min_dist = dist\r\n",
        "            break\r\n",
        "        if min_dist >= dstep:\r\n",
        "          pos_clicks[i][0] = point[0]\r\n",
        "          pos_clicks[i][1] = point[1]\r\n",
        "          resample = 0\r\n",
        "  #kmediods = KMedoids(n)\r\n",
        "  #kmediods.fit(points.numpy())\r\n",
        "  #pos_clicks = torch.tensor(kmediods.cluster_centers_)\r\n",
        "  return pos_clicks\r\n",
        "\r\n",
        "# Generate n -ve clicks by randomly sampling points in the background\r\n",
        "def gen_neg_clicks(target, n):\r\n",
        "  neg_clicks = torch.zeros((n, 2))\r\n",
        "  dstep = 5\r\n",
        "  dmargin_min = 5\r\n",
        "  dmargin_max = 20\r\n",
        "\r\n",
        "  target_inv = torch.logical_not(target[0])\r\n",
        "  distances = ndimage.distance_transform_edt(target_inv.numpy())\r\n",
        "  distances = np.where(distances < dmargin_min, 0, distances)\r\n",
        "  distances = np.where(distances > dmargin_max, 0, distances)\r\n",
        "  points = torch.nonzero(torch.from_numpy(distances))\r\n",
        "  print(len(points))\r\n",
        "\r\n",
        "  for i in range(n):\r\n",
        "    resample = 1\r\n",
        "    while resample > 0:\r\n",
        "      index = random.randint(0, len(points))\r\n",
        "      point = points[index]\r\n",
        "      if len(neg_clicks) == 0:\r\n",
        "        neg_clicks[i][0] = point[0]\r\n",
        "        neg_clicks[i][1] = point[1]\r\n",
        "        resample = 0\r\n",
        "      else:\r\n",
        "        min_dist = dstep\r\n",
        "        for neg in neg_clicks:\r\n",
        "          dist = torch.dist(point.type(torch.FloatTensor), neg.type(torch.FloatTensor), 2)\r\n",
        "          if dist < min_dist:\r\n",
        "            min_dist = dist\r\n",
        "            break\r\n",
        "        if min_dist >= dstep:\r\n",
        "          neg_clicks[i][0] = point[0]\r\n",
        "          neg_clicks[i][1] = point[1]\r\n",
        "          resample = 0\r\n",
        "\r\n",
        "  return neg_clicks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNOxZdX7vz13"
      },
      "source": [
        "# Convert clicks to interaction maps\r\n",
        "def convert_clicks(pos_clicks, neg_clicks):\r\n",
        "  pos_map = torch.ones((480, 480))\r\n",
        "  neg_map = torch.ones((480, 480))\r\n",
        "\r\n",
        "  for click in pos_clicks:\r\n",
        "    pos_map[int(click[0])][int(click[1])] = 0\r\n",
        "  for click in neg_clicks:\r\n",
        "    neg_map[int(click[0])][int(click[1])] = 0\r\n",
        "\r\n",
        "  pos_map = ndimage.distance_transform_edt(pos_map.numpy())\r\n",
        "  neg_map = ndimage.distance_transform_edt(neg_map.numpy())\r\n",
        "  pos_map = np.where(pos_map > 255, 255, pos_map)\r\n",
        "  neg_map = np.where(neg_map > 255, 255, neg_map)\r\n",
        "  pos_map = torch.from_numpy(pos_map).unsqueeze(0)\r\n",
        "  neg_map = torch.from_numpy(neg_map).unsqueeze(0)\r\n",
        "\r\n",
        "  return pos_map, neg_map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idazCHrFCiU3"
      },
      "source": [
        "# Code to test click generation by ploting clicks and segmentation mask\r\n",
        "img, target = sbd.__getitem__(6)\r\n",
        "pos_clicks = gen_pos_clicks(target, 5)\r\n",
        "print(pos_clicks)\r\n",
        "neg_clicks = gen_neg_clicks(target, 3)\r\n",
        "print(neg_clicks)\r\n",
        "plt.imshow(transforms.ToPILImage()(target))\r\n",
        "plt.scatter(pos_clicks[:, 1], pos_clicks[:, 0])\r\n",
        "plt.scatter(neg_clicks[:, 1], neg_clicks[:, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FpeQGf_v73r"
      },
      "source": [
        "# Code to visualize interaction maps\r\n",
        "pos_map, neg_map = convert_clicks(pos_clicks, neg_clicks)\r\n",
        "fig,axes = plt.subplots(nrows = 1, ncols = 2)\r\n",
        "axes[0].imshow(pos_map[0].numpy(), cmap='gray', interpolation='bicubic')\r\n",
        "axes[1].imshow(neg_map[0].numpy(), cmap='gray', interpolation='bicubic')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dKzk186FYEV"
      },
      "source": [
        "\"\"\"\r\n",
        "DenseNet Encoder Network\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "class _DenseLayer(nn.Module):\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        num_input_features: int,\r\n",
        "        growth_rate: int,\r\n",
        "        bn_size: int,\r\n",
        "        drop_rate: float,\r\n",
        "        memory_efficient: bool = False\r\n",
        "    ) -> None:\r\n",
        "        super(_DenseLayer, self).__init__()\r\n",
        "        self.norm1: nn.BatchNorm2d\r\n",
        "        self.add_module('norm1', nn.BatchNorm2d(num_input_features))\r\n",
        "        self.relu1: nn.ReLU\r\n",
        "        self.add_module('relu1', nn.ReLU(inplace=True))\r\n",
        "        self.conv1: nn.Conv2d\r\n",
        "        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\r\n",
        "                                           growth_rate, kernel_size=1, stride=1,\r\n",
        "                                           bias=False))\r\n",
        "        self.norm2: nn.BatchNorm2d\r\n",
        "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate))\r\n",
        "        self.relu2: nn.ReLU\r\n",
        "        self.add_module('relu2', nn.ReLU(inplace=True))\r\n",
        "        self.conv2: nn.Conv2d\r\n",
        "        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\r\n",
        "                                           kernel_size=3, stride=1, padding=1,\r\n",
        "                                           bias=False))\r\n",
        "        self.drop_rate = float(drop_rate)\r\n",
        "        self.memory_efficient = memory_efficient\r\n",
        "\r\n",
        "    def bn_function(self, inputs: List[Tensor]) -> Tensor:\r\n",
        "        concated_features = torch.cat(inputs, 1)\r\n",
        "        bottleneck_output = self.conv1(self.relu1(self.norm1(concated_features)))  # noqa: T484\r\n",
        "        return bottleneck_output\r\n",
        "\r\n",
        "    # todo: rewrite when torchscript supports any\r\n",
        "    def any_requires_grad(self, input: List[Tensor]) -> bool:\r\n",
        "        for tensor in input:\r\n",
        "            if tensor.requires_grad:\r\n",
        "                return True\r\n",
        "        return False\r\n",
        "\r\n",
        "    @torch.jit.unused  # noqa: T484\r\n",
        "    def call_checkpoint_bottleneck(self, input: List[Tensor]) -> Tensor:\r\n",
        "        def closure(*inputs):\r\n",
        "            return self.bn_function(inputs)\r\n",
        "\r\n",
        "        return cp.checkpoint(closure, *input)\r\n",
        "\r\n",
        "    @torch.jit._overload_method  # noqa: F811\r\n",
        "    def forward(self, input: List[Tensor]) -> Tensor:\r\n",
        "        pass\r\n",
        "\r\n",
        "    @torch.jit._overload_method  # noqa: F811\r\n",
        "    def forward(self, input: Tensor) -> Tensor:\r\n",
        "        pass\r\n",
        "\r\n",
        "    # torchscript does not yet support *args, so we overload method\r\n",
        "    # allowing it to take either a List[Tensor] or single Tensor\r\n",
        "    def forward(self, input: Tensor) -> Tensor:  # noqa: F811\r\n",
        "        if isinstance(input, Tensor):\r\n",
        "            prev_features = [input]\r\n",
        "        else:\r\n",
        "            prev_features = input\r\n",
        "\r\n",
        "        if self.memory_efficient and self.any_requires_grad(prev_features):\r\n",
        "            if torch.jit.is_scripting():\r\n",
        "                raise Exception(\"Memory Efficient not supported in JIT\")\r\n",
        "\r\n",
        "            bottleneck_output = self.call_checkpoint_bottleneck(prev_features)\r\n",
        "        else:\r\n",
        "            bottleneck_output = self.bn_function(prev_features)\r\n",
        "\r\n",
        "        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\r\n",
        "        if self.drop_rate > 0:\r\n",
        "            new_features = F.dropout(new_features, p=self.drop_rate,\r\n",
        "                                     training=self.training)\r\n",
        "        return new_features\r\n",
        "\r\n",
        "\r\n",
        "class _DenseBlock(nn.ModuleDict):\r\n",
        "    _version = 2\r\n",
        "\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        num_layers: int,\r\n",
        "        num_input_features: int,\r\n",
        "        bn_size: int,\r\n",
        "        growth_rate: int,\r\n",
        "        drop_rate: float,\r\n",
        "        memory_efficient: bool = False\r\n",
        "    ) -> None:\r\n",
        "        super(_DenseBlock, self).__init__()\r\n",
        "        for i in range(num_layers):\r\n",
        "            layer = _DenseLayer(\r\n",
        "                num_input_features + i * growth_rate,\r\n",
        "                growth_rate=growth_rate,\r\n",
        "                bn_size=bn_size,\r\n",
        "                drop_rate=drop_rate,\r\n",
        "                memory_efficient=memory_efficient,\r\n",
        "            )\r\n",
        "            self.add_module('denselayer%d' % (i + 1), layer)\r\n",
        "\r\n",
        "    def forward(self, init_features: Tensor) -> Tensor:\r\n",
        "        features = [init_features]\r\n",
        "        for name, layer in self.items():\r\n",
        "            new_features = layer(features)\r\n",
        "            features.append(new_features)\r\n",
        "        return torch.cat(features, 1)\r\n",
        "\r\n",
        "\r\n",
        "class _Transition(nn.Sequential):\r\n",
        "    def __init__(self, num_input_features: int, num_output_features: int) -> None:\r\n",
        "        super(_Transition, self).__init__()\r\n",
        "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\r\n",
        "        self.add_module('relu', nn.ReLU(inplace=True))\r\n",
        "        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\r\n",
        "                                          kernel_size=1, stride=1, bias=False))\r\n",
        "        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\r\n",
        "\r\n",
        "\r\n",
        "class DenseNet(nn.Module):\r\n",
        "    \"\"\"Densenet-BC model class, based on\r\n",
        "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_.\r\n",
        "    Args:\r\n",
        "        growth_rate (int) - how many filters to add each layer (`k` in paper)\r\n",
        "        block_config (list of 4 ints) - how many layers in each pooling block\r\n",
        "        num_init_features (int) - the number of filters to learn in the first convolution layer\r\n",
        "        bn_size (int) - multiplicative factor for number of bottle neck layers\r\n",
        "          (i.e. bn_size * k features in the bottleneck layer)\r\n",
        "        drop_rate (float) - dropout rate after each dense layer\r\n",
        "        num_classes (int) - number of classification classes\r\n",
        "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\r\n",
        "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        growth_rate: int = 32,\r\n",
        "        block_config: Tuple[int, int, int, int] = (6, 12, 24, 16),\r\n",
        "        num_init_features: int = 64,\r\n",
        "        bn_size: int = 4,\r\n",
        "        drop_rate: float = 0,\r\n",
        "        num_classes: int = 1000,\r\n",
        "        memory_efficient: bool = False\r\n",
        "    ) -> None:\r\n",
        "\r\n",
        "        super(DenseNet, self).__init__()\r\n",
        "\r\n",
        "        # First convolution\r\n",
        "        self.features = nn.Sequential(OrderedDict([\r\n",
        "            ('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2,\r\n",
        "                                padding=3, bias=False)),\r\n",
        "            ('norm0', nn.BatchNorm2d(num_init_features)),\r\n",
        "            ('relu0', nn.ReLU(inplace=True)),\r\n",
        "            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\r\n",
        "        ]))\r\n",
        "\r\n",
        "        # Each denseblock\r\n",
        "        num_features = num_init_features\r\n",
        "        for i, num_layers in enumerate(block_config):\r\n",
        "            block = _DenseBlock(\r\n",
        "                num_layers=num_layers,\r\n",
        "                num_input_features=num_features,\r\n",
        "                bn_size=bn_size,\r\n",
        "                growth_rate=growth_rate,\r\n",
        "                drop_rate=drop_rate,\r\n",
        "                memory_efficient=memory_efficient\r\n",
        "            )\r\n",
        "            self.features.add_module('denseblock%d' % (i + 1), block)\r\n",
        "            num_features = num_features + num_layers * growth_rate\r\n",
        "            if i != len(block_config) - 1:\r\n",
        "                trans = _Transition(num_input_features=num_features,\r\n",
        "                                    num_output_features=num_features // 2)\r\n",
        "                self.features.add_module('transition%d' % (i + 1), trans)\r\n",
        "                num_features = num_features // 2\r\n",
        "\r\n",
        "        # Final batch norm\r\n",
        "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\r\n",
        "\r\n",
        "        # Linear layer\r\n",
        "        self.classifier = nn.Linear(num_features, num_classes)\r\n",
        "\r\n",
        "        # Official init from torch repo.\r\n",
        "        for m in self.modules():\r\n",
        "            if isinstance(m, nn.Conv2d):\r\n",
        "                nn.init.kaiming_normal_(m.weight)\r\n",
        "            elif isinstance(m, nn.BatchNorm2d):\r\n",
        "                nn.init.constant_(m.weight, 1)\r\n",
        "                nn.init.constant_(m.bias, 0)\r\n",
        "            elif isinstance(m, nn.Linear):\r\n",
        "                nn.init.constant_(m.bias, 0)\r\n",
        "\r\n",
        "    def forward(self, x: Tensor) -> Tensor:\r\n",
        "        # First Convolution\r\n",
        "        x = self.features.conv0(x)\r\n",
        "        x = self.features.norm0(x)\r\n",
        "        x = self.features.relu0(x)\r\n",
        "        x = self.features.pool0(x)\r\n",
        "\r\n",
        "        # Dense Blocks\r\n",
        "        dense1_out = self.features.denseblock1(x)\r\n",
        "        x = self.features.transition1(dense1_out)\r\n",
        "        dense2_out = self.features.denseblock2(x)\r\n",
        "        x = self.features.transition2(dense2_out)\r\n",
        "        dense3_out = self.features.denseblock3(x)\r\n",
        "        x = self.features.transition3(dense3_out)\r\n",
        "        dense4_out = self.features.denseblock4(x)\r\n",
        "        \r\n",
        "        out = self.features.norm5(dense4_out)\r\n",
        "        out = F.relu(out, inplace=True)\r\n",
        "        out = F.adaptive_avg_pool2d(out, (1, 1))\r\n",
        "        out = torch.flatten(out, 1)\r\n",
        "        out = self.classifier(out)\r\n",
        "\r\n",
        "        dense_out = [dense1_out, dense2_out, dense3_out, dense4_out]\r\n",
        "        return out, dense_out\r\n",
        "\r\n",
        "\r\n",
        "def _load_state_dict(model: nn.Module, model_url: str, progress: bool) -> None:\r\n",
        "    # '.'s are no longer allowed in module names, but previous _DenseLayer\r\n",
        "    # has keys 'norm.1', 'relu.1', 'conv.1', 'norm.2', 'relu.2', 'conv.2'.\r\n",
        "    # They are also in the checkpoints in model_urls. This pattern is used\r\n",
        "    # to find such keys.\r\n",
        "    pattern = re.compile(\r\n",
        "        r'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$')\r\n",
        "\r\n",
        "    state_dict = load_state_dict_from_url(model_url, progress=progress)\r\n",
        "    for key in list(state_dict.keys()):\r\n",
        "        res = pattern.match(key)\r\n",
        "        if res:\r\n",
        "            new_key = res.group(1) + res.group(2)\r\n",
        "            state_dict[new_key] = state_dict[key]\r\n",
        "            del state_dict[key]\r\n",
        "    model.load_state_dict(state_dict)\r\n",
        "\r\n",
        "\r\n",
        "def _densenet(\r\n",
        "    arch: str,\r\n",
        "    growth_rate: int,\r\n",
        "    block_config: Tuple[int, int, int, int],\r\n",
        "    num_init_features: int,\r\n",
        "    pretrained: bool,\r\n",
        "    progress: bool,\r\n",
        "    **kwargs: Any\r\n",
        ") -> DenseNet:\r\n",
        "    model = DenseNet(growth_rate, block_config, num_init_features, **kwargs)\r\n",
        "    if pretrained:\r\n",
        "        _load_state_dict(model, model_urls[arch], progress)\r\n",
        "    return model\r\n",
        "\r\n",
        "\r\n",
        "def densenet121(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> DenseNet:\r\n",
        "    \"\"\"Densenet-121 model from\r\n",
        "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_.\r\n",
        "    Args:\r\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\r\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\r\n",
        "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\r\n",
        "          but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_.\r\n",
        "    \"\"\"\r\n",
        "    return _densenet('densenet121', 32, (6, 12, 24, 16), 64, pretrained, progress,\r\n",
        "                     **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqUPHUtsIsKE"
      },
      "source": [
        "\"\"\"\r\n",
        "Decoder Network\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "_weights_dict = dict()\r\n",
        "\r\n",
        "class Decoder(nn.Module):\r\n",
        "\r\n",
        "  def __init__(self):\r\n",
        "    super(Decoder, self).__init__()\r\n",
        "\r\n",
        "    self.conv_SE_1_32_1 = self.__conv(2, name='conv_SE_1/32_1', in_channels=1024, out_channels=64, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=True)\r\n",
        "    self.conv_SE_1_32_2 = self.__conv(2, name='conv_SE_1/32_2', in_channels=64, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=True)\r\n",
        "    self.conv_1_32_1d = self.__conv(2, name='conv_1/32_1d', in_channels=1024, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=False)\r\n",
        "    self.prelu_1_32_1d = nn.PReLU(num_parameters=512)\r\n",
        "    self.bn_1_32_1d = self.__batch_normalization(2, 'bn_1/32_1d', num_features=512, eps=9.999999747378752e-06, momentum=0.0)\r\n",
        "    self.conv_1_32_2d = self.__conv(2, name='conv_1/32_2d', in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=False)\r\n",
        "    self.prelu_1_32_2d = nn.PReLU(num_parameters=512)\r\n",
        "    self.bn_1_32_2d = self.__batch_normalization(2, 'bn_1/32_2d', num_features=512, eps=9.999999747378752e-06, momentum=0.0)\r\n",
        "    self.conv_1_32_3d = self.__conv(2, name='conv_1/32_3d', in_channels=512, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)\r\n",
        "    self.prelu_1_32_3d = nn.PReLU(num_parameters=256)\r\n",
        "    self.bn_1_32_3d = self.__batch_normalization(2, 'bn_1/32_3d', num_features=256, eps=9.999999747378752e-06, momentum=0.0)\r\n",
        "    self.deconv_1_16d = nn.ConvTranspose2d(in_channels=256, out_channels=256, kernel_size=2, stride=2, bias=False)\r\n",
        "    self.prelu_1_16d = nn.PReLU(num_parameters=256)\r\n",
        "    self.bn_1_16d = self.__batch_normalization(2, 'bn_1/16d', num_features=256, eps=9.999999747378752e-06, momentum=0.0)\r\n",
        "    self.conv_SE_1_16_1 = self.__conv(2, name='conv_SE_1/16_1', in_channels=1024, out_channels=64, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)\r\n",
        "    self.conv_SE_1_16_2 = self.__conv(2, name='conv_SE_1/16_2', in_channels=64, out_channels=1024, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)\r\n",
        "    self.conv_SE_1_16 = self.__conv(2, name='conv_SE_1/16', in_channels=1024, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)\r\n",
        "    self.prelu_SE_1_16 = nn.PReLU(num_parameters=256)\r\n",
        "    self.bn_SE_1_16 = self.__batch_normalization(2, 'bn_SE_1/16d', num_features=256, eps=9.999999747378752e-06, momentum=0.0)\r\n",
        "\r\n",
        "    self.conv_1_16_1d = self.__conv(2, name='conv_1/16_1d', in_channels=512, out_channels=256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=False)\r\n",
        "    self.prelu_1_16_1d = nn.PReLU(num_parameters=256)\r\n",
        "    self.bn_1_16_1d = self.__batch_normalization(2, 'bn_1/16_1d', num_features=256, eps=9.999999747378752e-06, momentum=0.0)\r\n",
        "    self.conv_1_16_2d = self.__conv(2, name='conv_1/16_2d', in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=False)\r\n",
        "    self.prelu_1_16_2d = nn.PReLU(num_parameters=256)\r\n",
        "    self.bn_1_16_2d = self.__batch_normalization(2, 'bn_1/16_2d', num_features=256, eps=9.999999747378752e-06, momentum=0.0)\r\n",
        "    self.conv_1_16_3d = self.__conv(2, name='conv_1/16_3d', in_channels=256, out_channels=128, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)\r\n",
        "    self.prelu_1_16_3d = nn.PReLU(num_parameters=128)\r\n",
        "    self.bn_1_16_3d = self.__batch_normalization(2, 'bn_1/16_3d', num_features=128, eps=9.999999747378752e-06, momentum=0.0)\r\n",
        "    self.deconv_1_8d = nn.ConvTranspose2d(in_channels=128, out_channels=128, kernel_size=2, stride=2, bias=False)\r\n",
        "    self.prelu_1_8d = nn.PReLU(num_parameters=128)\r\n",
        "    self.bn_1_8d = self.__batch_normalization(2, 'bn_1/8d', num_features=128, eps=9.999999747378752e-06, momentum=0.0)\r\n",
        "    self.conv_SE_1_8_1 = self.__conv(2, name='conv_SE_1/8_1', in_channels=512, out_channels=32, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)\r\n",
        "    self.conv_SE_1_8_2 = self.__conv(2, name='conv_SE_1/8_1', in_channels=32, out_channels=512, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)\r\n",
        "    self.conv_SE_1_8 = self.__conv(2, name='conv_SE_1/8', in_channels=512, out_channels=128, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)\r\n",
        "    self.prelu_SE_1_8 = nn.PReLU(num_parameters=128)\r\n",
        "    self.bn_SE_1_8 = self.__batch_normalization(2, 'bn_SE_1/8d', num_features=128, eps=9.999999747378752e-06, momentum=0.0)\r\n",
        "\r\n",
        "    self.conv_1_8_1d = self.__conv(2, name='conv_1/8_1d', in_channels=256, out_channels=128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=False)\r\n",
        "    self.prelu_1_8_1d = nn.PReLU(num_parameters=128)\r\n",
        "    self.bn_1_8_1d = self.__batch_normalization(2, 'bn_1/8_1d', num_features=128, eps=9.999999747378752e-06, momentum=0.0)\r\n",
        "    self.conv_1_8_2d = self.__conv(2, name='conv_1/8_2d', in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=False)\r\n",
        "    self.prelu_1_8_2d = nn.PReLU(num_parameters=128)\r\n",
        "    self.bn_1_8_2d = self.__batch_normalization(2, 'bn_1/8_2d', num_features=128, eps=9.999999747378752e-06, momentum=0.0)\r\n",
        "    self.conv_1_8_3d = self.__conv(2, name='conv_1/8_3d', in_channels=128, out_channels=64, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)\r\n",
        "    self.prelu_1_8_3d = nn.PReLU(num_parameters=64)\r\n",
        "    self.bn_1_8_3d = self.__batch_normalization(2, 'bn_1/8_3d', num_features=64, eps=9.999999747378752e-06, momentum=0.0)\r\n",
        "    self.deconv_1_4d = nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=2, stride=2, bias=False)\r\n",
        "    self.prelu_1_4d = nn.PReLU(num_parameters=64)\r\n",
        "    self.bn_1_4d = self.__batch_normalization(2, 'bn_1/4d', num_features=64, eps=9.999999747378752e-06, momentum=0.0)\r\n",
        "    self.conv_SE_1_4_1 = self.__conv(2, name='conv_SE_1/4_1', in_channels=256, out_channels=16, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)\r\n",
        "    self.conv_SE_1_4_2 = self.__conv(2, name='conv_SE_1/4_2', in_channels=16, out_channels=256, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)\r\n",
        "    self.conv_SE_1_4 = self.__conv(2, name='conv_SE_1/4', in_channels=256, out_channels=64, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)\r\n",
        "    self.prelu_SE_1_4 = nn.PReLU(num_parameters=64)\r\n",
        "    self.bn_SE_1_4 = self.__batch_normalization(2, 'bn_1/4d', num_features=64, eps=9.999999747378752e-06, momentum=0.0)\r\n",
        "\r\n",
        "    self.conv_1_4_1d = self.__conv(2, name='conv_1/4_1d', in_channels=128, out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=False)\r\n",
        "    self.prelu_1_4_1d = nn.PReLU(num_parameters=64)\r\n",
        "    self.bn_1_4_1d = self.__batch_normalization(2, 'bn_1/4_1d', num_features=64, eps=9.999999747378752e-06, momentum=0.0)\r\n",
        "    self.conv_1_4_2d = self.__conv(2, name='conv_1/4_2d', in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=False)\r\n",
        "    self.prelu_1_4_2d = nn.PReLU(num_parameters=64)\r\n",
        "    self.bn_1_4_2d = self.__batch_normalization(2, 'bn_1/4_2d', num_features=64, eps=9.999999747378752e-06, momentum=0.0)\r\n",
        "    self.conv_1_4_3d = self.__conv(2, name='conv_1/4_3d', in_channels=64, out_channels=32, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)\r\n",
        "    self.prelu_1_4_3d = nn.PReLU(num_parameters=32)\r\n",
        "    self.bn_1_4_3d = self.__batch_normalization(2, 'bn_1/4_3d', num_features=32, eps=9.999999747378752e-06, momentum=0.0)\r\n",
        "\r\n",
        "    self.pred_1_4 = self.__conv(2, name='pred_1/4', in_channels=32, out_channels=1, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=True)\r\n",
        "    self.pred_step_1 = nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=4, stride=4, bias=False)\r\n",
        "\r\n",
        "    self.conv_atrous1_1 = self.__conv(2, name='conv_atrous1_1', in_channels=6, out_channels=32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=False)\r\n",
        "    self.prelu_atrous1_1 = nn.PReLU(num_parameters=32)\r\n",
        "    self.bn_atrous1_1 = self.__batch_normalization(2, 'bn_atrous1_1', num_features=32, eps=9.999999747378752e-06, momentum=0.0)\r\n",
        "    self.conv_atrous1_2 = self.__conv(2, name='conv_atrous1_2', in_channels=32, out_channels=32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=False)\r\n",
        "    self.prelu_atrous1_2 = nn.PReLU(num_parameters=32)\r\n",
        "    self.bn_atrous1_2 = self.__batch_normalization(2, 'bn_atrous1_2', num_features=32, eps=9.999999747378752e-06, momentum=0.0)\r\n",
        "    self.conv_atrous1_3 = self.__conv(2, name='conv_atrous1_3', in_channels=32, out_channels=16, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)\r\n",
        "    self.prelu_atrous1_3 = nn.PReLU(num_parameters=16)\r\n",
        "    self.bn_atrous1_3 = self.__batch_normalization(2, 'bn_atrous1_3', num_features=16, eps=9.999999747378752e-06, momentum=0.0)\r\n",
        "\r\n",
        "    self.conv_atrous2_1 = self.__conv(2, name='conv_atrous2_1', in_channels=6, out_channels=32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=1, bias=False)\r\n",
        "    self.prelu_atrous2_1 = nn.PReLU(num_parameters=32)\r\n",
        "    self.bn_atrous2_1 = self.__batch_normalization(2, 'bn_atrous2_1', num_features=32, eps=9.999999747378752e-06, momentum=0.0)\r\n",
        "    self.conv_atrous2_2 = self.__conv(2, name='conv_atrous2_2', in_channels=32, out_channels=32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=False)\r\n",
        "    self.prelu_atrous2_2 = nn.PReLU(num_parameters=32)\r\n",
        "    self.bn_atrous2_2 = self.__batch_normalization(2, 'bn_atrous2_2', num_features=32, eps=9.999999747378752e-06, momentum=0.0)\r\n",
        "    self.conv_atrous2_3 = self.__conv(2, name='conv_atrous2_3', in_channels=32, out_channels=16, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)\r\n",
        "    self.prelu_atrous2_3 = nn.PReLU(num_parameters=16)\r\n",
        "    self.bn_atrous2_3 = self.__batch_normalization(2, 'bn_atrous2_3', num_features=16, eps=9.999999747378752e-06, momentum=0.0)\r\n",
        "\r\n",
        "    self.conv_atrous3_1 = self.__conv(2, name='conv_atrous3_1', in_channels=6, out_channels=32, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), dilation=(3, 3), groups=1, bias=False)\r\n",
        "    self.prelu_atrous3_1 = nn.PReLU(num_parameters=32)\r\n",
        "    self.bn_atrous3_1 = self.__batch_normalization(2, 'bn_atrous3_1', num_features=32, eps=9.999999747378752e-06, momentum=0.0)\r\n",
        "    self.conv_atrous3_2 = self.__conv(2, name='conv_atrous3_2', in_channels=32, out_channels=32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=False)\r\n",
        "    self.prelu_atrous3_2 = nn.PReLU(num_parameters=32)\r\n",
        "    self.bn_atrous3_2 = self.__batch_normalization(2, 'bn_atrous3_2', num_features=32, eps=9.999999747378752e-06, momentum=0.0)\r\n",
        "    self.conv_atrous3_3 = self.__conv(2, name='conv_atrous3_3', in_channels=32, out_channels=16, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=False)\r\n",
        "    self.prelu_atrous3_3 = nn.PReLU(num_parameters=16)\r\n",
        "    self.bn_atrous3_3 = self.__batch_normalization(2, 'bn_atrous3_3', num_features=16, eps=9.999999747378752e-06, momentum=0.0)\r\n",
        "    \r\n",
        "    self.conv_s2_down = self.__conv(2, name='conv_s2_down', in_channels=48, out_channels=3, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=True)\r\n",
        "    self.conv_s2_up = self.__conv(2, name='conv_s2_up', in_channels=3, out_channels=48, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=True)\r\n",
        "    self.conv_p1_1 = self.__conv(2, name='conv_p1_1', in_channels=48, out_channels=16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=False)\r\n",
        "    self.prelu_p1_1 = nn.PReLU(num_parameters=16)\r\n",
        "    self.bn_p1_1 = self.__batch_normalization(2, 'bn_p1_1', num_features=16, eps=9.999999747378752e-06, momentum=0.0)\r\n",
        "    self.pred_step_2 = self.__conv(2, name='pred_step_2', in_channels=16, out_channels=1, kernel_size=(1, 1), stride=(1, 1), groups=1, bias=True)\r\n",
        "\r\n",
        "  def forward(self, dense_out):\r\n",
        "    concat_input = dense_out[4]\r\n",
        "    concat_5_16 = dense_out[3]\r\n",
        "    concat_4_24 = dense_out[2]\r\n",
        "    concat_3_12 = dense_out[1]\r\n",
        "    concat_2_6 = dense_out[0]\r\n",
        "\r\n",
        "    ################ BLOCK 1 ################      \r\n",
        "    pool_SE_1_32    = F.avg_pool2d(concat_5_16, kernel_size=(15, 15), stride=(1, 1), padding=(0,), ceil_mode=False, count_include_pad=False)\r\n",
        "    conv_SE_1_32_1  = self.conv_SE_1_32_1(pool_SE_1_32)\r\n",
        "    relu_SE_1_32_1  = F.relu(conv_SE_1_32_1)\r\n",
        "    conv_SE_1_32_2  = self.conv_SE_1_32_2(relu_SE_1_32_1)\r\n",
        "    sigm_SE_1_32    = torch.sigmoid(conv_SE_1_32_2)\r\n",
        "    reshape_SE_1_32 = torch.reshape(input = sigm_SE_1_32, shape = (1,1024,1,1))\r\n",
        "    scale_SE_1_32   = concat_5_16 * reshape_SE_1_32\r\n",
        "\r\n",
        "    conv_1_32_1d    = self.conv_1_32_1d(scale_SE_1_32)\r\n",
        "    #prelu_1_32_1d   = F.prelu(conv_1_32_1d, torch.from_numpy(_weights_dict['prelu_1/32_1d']['weights']))\r\n",
        "    prelu_1_32_1d   = self.prelu_1_32_1d(conv_1_32_1d)\r\n",
        "    bn_1_32_1d      = self.bn_1_32_1d(prelu_1_32_1d)\r\n",
        "\r\n",
        "    conv_1_32_2d    = self.conv_1_32_2d(bn_1_32_1d)\r\n",
        "    #prelu_1_32_2d   = F.prelu(conv_1_32_2d, torch.from_numpy(_weights_dict['prelu_1/32_2d']['weights']))\r\n",
        "    prelu_1_32_2d   = self.prelu_1_32_2d(conv_1_32_2d)\r\n",
        "    bn_1_32_2d      = self.bn_1_32_2d(prelu_1_32_2d)\r\n",
        "\r\n",
        "    conv_1_32_3d    = self.conv_1_32_3d(bn_1_32_2d)\r\n",
        "    #prelu_1_32_3d   = F.prelu(conv_1_32_3d, torch.from_numpy(_weights_dict['prelu_1/32_3d']['weights']))\r\n",
        "    prelu_1_32_3d   = self.prelu_1_32_3d(conv_1_32_3d)\r\n",
        "    bn_1_32_3d      = self.bn_1_32_3d(prelu_1_32_3d)\r\n",
        "\r\n",
        "    deconv_1_16d\t  = self.deconv_1_16d(bn_1_32_3d)\r\n",
        "    #prelu_1_16d     = F.prelu(deconv_1_16d, torch.from_numpy(_weights_dict['prelu_1/16d']['weights']))\r\n",
        "    prelu_1_16d     = self.prelu_1_16d(deconv_1_16d)\r\n",
        "    bn_1_16d        = self.bn_1_16d(prelu_1_16d)\r\n",
        "\r\n",
        "    pool_SE_1_16    = F.avg_pool2d(concat_4_24, kernel_size=(30, 30), stride=(1, 1), padding=(0,), ceil_mode=False, count_include_pad=False)\r\n",
        "    conv_SE_1_16_1  = self.conv_SE_1_16_1(pool_SE_1_16)\r\n",
        "    relu_SE_1_16_1  = F.relu(conv_SE_1_16_1)\r\n",
        "    conv_SE_1_16_2  = self.conv_SE_1_16_2(relu_SE_1_16_1)\r\n",
        "    sigm_SE_1_16    = torch.sigmoid(conv_SE_1_16_2)\r\n",
        "    reshape_SE_1_16 = torch.reshape(input = sigm_SE_1_16, shape = (1,1024,1,1))\r\n",
        "    scale_SE_1_16   = concat_4_24 * reshape_SE_1_16\r\n",
        "    conv_SE_1_16    = self.conv_SE_1_16(scale_SE_1_16)\r\n",
        "    #prelu_SE_1_16    = F.prelu(conv_SE_1_8, torch.from_numpy(_weights_dict['prelu_SE_1/16']['weights']))\r\n",
        "    prelu_SE_1_16   = self.prelu_SE_1_16(conv_SE_1_16)\r\n",
        "    bn_SE_1_16      = self.bn_SE_1_16(prelu_SE_1_16)\r\n",
        "    #########################################\r\n",
        "\r\n",
        "    ################ BLOCK 2 ################\r\n",
        "    concat_1_16d    = torch.cat((bn_1_16d, bn_SE_1_16,), 1)\r\n",
        "\r\n",
        "    conv_1_16_1d    = self.conv_1_16_1d(concat_1_16d)\r\n",
        "    #prelu_1_16_1d   = F.prelu(conv_1_16_1d, torch.from_numpy(_weights_dict['prelu_1/16_1d']['weights']))\r\n",
        "    prelu_1_16_1d   = self.prelu_1_16_1d(conv_1_16_1d)\r\n",
        "    bn_1_16_1d      = self.bn_1_16_1d(prelu_1_16_1d)\r\n",
        "\r\n",
        "    conv_1_16_2d    = self.conv_1_16_2d(bn_1_16_1d)\r\n",
        "    #prelu_1_16_2d   = F.prelu(conv_1_16_2d, torch.from_numpy(_weights_dict['prelu_1/16_2d']['weights']))\r\n",
        "    prelu_1_16_2d   = self.prelu_1_16_2d(conv_1_16_2d)\r\n",
        "    bn_1_16_2d      = self.bn_1_16_2d(prelu_1_16_2d)\r\n",
        "\r\n",
        "    conv_1_16_3d    = self.conv_1_16_3d(bn_1_16_2d)\r\n",
        "    #prelu_1_16_3d   = F.prelu(conv_1_16_3d, torch.from_numpy(_weights_dict['prelu_1/16_3d']['weights']))\r\n",
        "    prelu_1_16_3d   = self.prelu_1_16_3d(conv_1_16_3d)\r\n",
        "    bn_1_16_3d      = self.bn_1_16_3d(prelu_1_16_3d)\r\n",
        "\r\n",
        "    deconv_1_8d\t\t= self.deconv_1_8d(bn_1_16_3d)\r\n",
        "    #prelu_1_8d      = F.prelu(deconv_1_8d, torch.from_numpy(_weights_dict['prelu_1/8d']['weights']))\r\n",
        "    prelu_1_8d      = self.prelu_1_8d(deconv_1_8d)\r\n",
        "    bn_1_8d         = self.bn_1_8d(prelu_1_8d)\r\n",
        "\r\n",
        "    pool_SE_1_8     = F.avg_pool2d(concat_3_12, kernel_size=(60, 60), stride=(1, 1), padding=(0,), ceil_mode=False, count_include_pad=False)\r\n",
        "    conv_SE_1_8_1   = self.conv_SE_1_8_1(pool_SE_1_8)        \r\n",
        "    relu_SE_1_8_1   = F.relu(conv_SE_1_8_1)\r\n",
        "    conv_SE_1_8_2   = self.conv_SE_1_8_2(relu_SE_1_8_1)\r\n",
        "    sigm_SE_1_8     = torch.sigmoid(conv_SE_1_8_2)\r\n",
        "    reshape_SE_1_8  = torch.reshape(input = sigm_SE_1_8, shape = (1,512,1,1))\r\n",
        "    scale_SE_1_8    = concat_3_12 * reshape_SE_1_8\r\n",
        "    conv_SE_1_8     = self.conv_SE_1_8(scale_SE_1_8)\r\n",
        "    #prelu_SE_1_8    = F.prelu(conv_SE_1_8, torch.from_numpy(_weights_dict['prelu_SE_1/8']['weights']))\r\n",
        "    prelu_SE_1_8    = self.prelu_SE_1_8(conv_SE_1_8)\r\n",
        "    bn_SE_1_8       = self.bn_SE_1_8(prelu_SE_1_8)\r\n",
        "    #########################################\r\n",
        "\r\n",
        "    ################ BLOCK 3 ################\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\n",
        "    concat_1_8d     = torch.cat((bn_1_8d, bn_SE_1_8,), 1)\r\n",
        "\r\n",
        "    conv_1_8_1d     = self.conv_1_8_1d(concat_1_8d)\r\n",
        "    #prelu_1_8_1d    = F.prelu(conv_1_8_1d, torch.from_numpy(_weights_dict['prelu_1/8_1d']['weights']))\r\n",
        "    prelu_1_8_1d    = self.prelu_1_8_1d(conv_1_8_1d)\r\n",
        "    bn_1_8_1d       = self.bn_1_8_1d(prelu_1_8_1d)\r\n",
        "\r\n",
        "    conv_1_8_2d     = self.conv_1_8_2d(bn_1_8_1d)\r\n",
        "    #prelu_1_8_2d    = F.prelu(conv_1_8_2d, torch.from_numpy(_weights_dict['prelu_1/8_2d']['weights']))\r\n",
        "    prelu_1_8_2d    = self.prelu_1_8_2d(conv_1_8_2d)\r\n",
        "    bn_1_8_2d       = self.bn_1_8_2d(prelu_1_8_2d)\r\n",
        "\r\n",
        "    conv_1_8_3d     = self.conv_1_8_3d(bn_1_8_2d)\r\n",
        "    #prelu_1_8_3d    = F.prelu(conv_1_8_3d, torch.from_numpy(_weights_dict['prelu_1/8_3d']['weights']))\r\n",
        "    prelu_1_8_3d    = self.prelu_1_8_3d(conv_1_8_3d)\r\n",
        "    bn_1_8_3d       = self.bn_1_8_3d(prelu_1_8_3d)\r\n",
        "\r\n",
        "    deconv_1_4d\t\t= self.deconv_1_4d(bn_1_8_3d)\r\n",
        "    #prelu_1_4d      = F.prelu(deconv_1_4d, torch.from_numpy(_weights_dict['prelu_1/4d']['weights']))\r\n",
        "    prelu_1_4d      = self.prelu_1_4d(deconv_1_4d)\r\n",
        "    bn_1_4d         = self.bn_1_4d(prelu_1_4d)\r\n",
        "\r\n",
        "    pool_SE_1_4     = F.avg_pool2d(concat_2_6, kernel_size=(120, 120), stride=(1, 1), padding=(0,), ceil_mode=False, count_include_pad=False)\r\n",
        "    conv_SE_1_4_1   = self.conv_SE_1_4_1(pool_SE_1_4)\r\n",
        "    relu_SE_1_4_1   = F.relu(conv_SE_1_4_1)\r\n",
        "    conv_SE_1_4_2   = self.conv_SE_1_4_2(relu_SE_1_4_1)\r\n",
        "    sigm_SE_1_4     = torch.sigmoid(conv_SE_1_4_2)\r\n",
        "    reshape_SE_1_4  = torch.reshape(input = sigm_SE_1_4, shape = (1,256,1,1))\r\n",
        "    scale_SE_1_4    = concat_2_6 * reshape_SE_1_4\r\n",
        "    conv_SE_1_4     = self.conv_SE_1_4(scale_SE_1_4)\r\n",
        "    #prelu_SE_1_4    = F.prelu(conv_SE_1_4, torch.from_numpy(_weights_dict['prelu_SE_1/4']['weights']))\r\n",
        "    prelu_SE_1_4    = self.prelu_SE_1_4(conv_SE_1_4)\r\n",
        "    bn_SE_1_4       = self.bn_SE_1_4(prelu_SE_1_4)\r\n",
        "    #########################################\r\n",
        "\r\n",
        "    ################ BLOCK 4 ################\t\t\t\t\t\t\t\t\t\t\t\t\t\t        \r\n",
        "    concat_1_4d     = torch.cat((bn_1_4d, bn_SE_1_4,), 1)\r\n",
        "\r\n",
        "    conv_1_4_1d     = self.conv_1_4_1d(concat_1_4d)\r\n",
        "    #prelu_1_4_1d    = F.prelu(conv_1_4_1d, torch.from_numpy(_weights_dict['prelu_1/4_1d']['weights']))\r\n",
        "    prelu_1_4_1d    = self.prelu_1_4_1d(conv_1_4_1d)\r\n",
        "    bn_1_4_1d       = self.bn_1_4_1d(prelu_1_4_1d)\r\n",
        "\r\n",
        "    conv_1_4_2d     = self.conv_1_4_2d(bn_1_4_1d)\r\n",
        "    #prelu_1_4_2d    = F.prelu(conv_1_4_2d, torch.from_numpy(_weights_dict['prelu_1/4_2d']['weights']))\r\n",
        "    prelu_1_4_2d    = self.prelu_1_4_2d(conv_1_4_2d)\r\n",
        "    bn_1_4_2d       = self.bn_1_4_2d(prelu_1_4_2d)\r\n",
        "\r\n",
        "    conv_1_4_3d     = self.conv_1_4_3d(bn_1_4_2d)\r\n",
        "    #prelu_1_4_3d    = F.prelu(conv_1_4_3d, torch.from_numpy(_weights_dict['prelu_1/4_3d']['weights']))\r\n",
        "    prelu_1_4_3d    = self.prelu_1_4_3d(conv_1_4_3d)\r\n",
        "    bn_1_4_3d       = self.bn_1_4_3d(prelu_1_4_3d)\r\n",
        "    #########################################\r\n",
        "\r\n",
        "    ################ PREDICTION AT 1/4 ################\r\n",
        "    pred_1_4        = self.pred_1_4(bn_1_4_3d)\r\n",
        "\r\n",
        "    ################ UNSAMPLE THE PREDICTION FROM 1/4 TO 1/1 ################\r\n",
        "    pred_step_1 \t= self.pred_step_1(pred_1_4)\r\n",
        "    sigp_step_1     = torch.sigmoid(pred_step_1)\r\n",
        "\r\n",
        "\r\n",
        "    ################ SECONDARY NETWORK (FINE DECODER) STARTS HERE ################\r\n",
        "    concat_step_1   = torch.cat((concat_input, sigp_step_1,), 1)\r\n",
        "\r\n",
        "    ################ ATROUS POOLING BLOCK 1 ################\r\n",
        "    conv_atrous1_1  = self.conv_atrous1_1(concat_step_1)\r\n",
        "    #prelu_atrous1_1 = F.prelu(conv_atrous1_1, torch.from_numpy(_weights_dict['prelu_atrous1_1']['weights']))\r\n",
        "    prelu_atrous1_1 = self.prelu_atrous1_1(conv_atrous1_1)\r\n",
        "    bn_atrous1_1    = self.bn_atrous1_1(prelu_atrous1_1)\r\n",
        "\r\n",
        "    conv_atrous1_2  = self.conv_atrous1_2(bn_atrous1_1)\r\n",
        "    #prelu_atrous1_2 = F.prelu(conv_atrous1_2, torch.from_numpy(_weights_dict['prelu_atrous1_2']['weights']))\r\n",
        "    prelu_atrous1_2 = self.prelu_atrous1_2(conv_atrous1_2)\r\n",
        "    bn_atrous1_2    = self.bn_atrous1_2(prelu_atrous1_2)\r\n",
        "\r\n",
        "    conv_atrous1_3  = self.conv_atrous1_3(bn_atrous1_2)\r\n",
        "    #prelu_atrous1_3 = F.prelu(conv_atrous1_3, torch.from_numpy(_weights_dict['prelu_atrous1_3']['weights']))\r\n",
        "    prelu_atrous1_3 = self.prelu_atrous1_3(conv_atrous1_3)\r\n",
        "    bn_atrous1_3    = self.bn_atrous1_3(prelu_atrous1_3)\r\n",
        "    ########################################################\r\n",
        "\r\n",
        "    ################ ATROUS POOLING BLOCK 2 ################\r\n",
        "    conv_atrous2_1  = self.conv_atrous2_1(concat_step_1)\r\n",
        "    #prelu_atrous2_1 = F.prelu(conv_atrous2_1, torch.from_numpy(_weights_dict['prelu_atrous2_1']['weights']))\r\n",
        "    prelu_atrous2_1 = self.prelu_atrous2_1(conv_atrous2_1)\r\n",
        "    bn_atrous2_1    = self.bn_atrous2_1(prelu_atrous2_1)\r\n",
        "\r\n",
        "    conv_atrous2_2  = self.conv_atrous2_2(bn_atrous2_1)\r\n",
        "    #prelu_atrous2_2 = F.prelu(conv_atrous2_2, torch.from_numpy(_weights_dict['prelu_atrous2_2']['weights']))\r\n",
        "    prelu_atrous2_2 = self.prelu_atrous2_2(conv_atrous2_2)\r\n",
        "    bn_atrous2_2    = self.bn_atrous2_2(prelu_atrous2_2)\r\n",
        "\r\n",
        "    conv_atrous2_3  = self.conv_atrous2_3(bn_atrous2_2)\r\n",
        "    #prelu_atrous2_3 = F.prelu(conv_atrous2_3, torch.from_numpy(_weights_dict['prelu_atrous2_3']['weights']))\r\n",
        "    prelu_atrous2_3 = self.prelu_atrous2_3(conv_atrous2_3)\r\n",
        "    bn_atrous2_3    = self.bn_atrous2_3(prelu_atrous2_3)\r\n",
        "    ########################################################\r\n",
        "\r\n",
        "    ################ ATROUS POOLING BLOCK 3 ################\r\n",
        "    conv_atrous3_1  = self.conv_atrous3_1(concat_step_1)\r\n",
        "    #prelu_atrous3_1 = F.prelu(conv_atrous3_1, torch.from_numpy(_weights_dict['prelu_atrous3_1']['weights']))\r\n",
        "    prelu_atrous3_1 = self.prelu_atrous3_1(conv_atrous3_1)\r\n",
        "    bn_atrous3_1    = self.bn_atrous3_1(prelu_atrous3_1)\r\n",
        "\r\n",
        "    conv_atrous3_2  = self.conv_atrous3_2(bn_atrous3_1)\r\n",
        "    #prelu_atrous3_2 = F.prelu(conv_atrous3_2, torch.from_numpy(_weights_dict['prelu_atrous3_2']['weights']))\r\n",
        "    prelu_atrous3_2 = self.prelu_atrous3_2(conv_atrous3_2)\r\n",
        "    bn_atrous3_2    = self.bn_atrous3_2(prelu_atrous3_2)\r\n",
        "\r\n",
        "    conv_atrous3_3  = self.conv_atrous3_3(bn_atrous3_2)\r\n",
        "    #prelu_atrous3_3 = F.prelu(conv_atrous3_3, torch.from_numpy(_weights_dict['prelu_atrous3_3']['weights']))\r\n",
        "    prelu_atrous3_3 = self.prelu_atrous3_3(conv_atrous3_3)\r\n",
        "    bn_atrous3_3    = self.bn_atrous3_3(prelu_atrous3_3)\r\n",
        "    ########################################################\r\n",
        "\r\n",
        "    ################ CONCAT + SQUEEZ & EXCITATION ################\r\n",
        "    concat_step_2   = torch.cat((bn_atrous1_3, bn_atrous2_3, bn_atrous3_3,), 1)\r\n",
        "    gpool_s2        = F.avg_pool2d(concat_step_2, kernel_size=(480, 480), stride=(1, 1), padding=(0,), ceil_mode=False, count_include_pad=False)\r\n",
        "    conv_s2_down    = self.conv_s2_down(gpool_s2)\r\n",
        "    relu_s2_down    = F.relu(conv_s2_down)\r\n",
        "    conv_s2_up      = self.conv_s2_up(relu_s2_down)\r\n",
        "    sig_s2_up       = torch.sigmoid(conv_s2_up)\r\n",
        "    resh_s2         = torch.reshape(input = sig_s2_up, shape = (1,48,1,1))\r\n",
        "    scale_s2        = concat_step_2 * resh_s2\r\n",
        "    ##############################################################\r\n",
        "\r\n",
        "    ################ PREDICTION ################\r\n",
        "    conv_p1_1       = self.conv_p1_1(scale_s2)\r\n",
        "    #prelu_p1_1      = F.prelu(conv_p1_1, torch.from_numpy(_weights_dict['prelu_p1_1']['weights']))\r\n",
        "    prelu_p1_1      = self.prelu_p1_1(conv_p1_1)\r\n",
        "    bn_p1_1         = self.bn_p1_1(prelu_p1_1)\r\n",
        "    pred_step_2     = self.pred_step_2(bn_p1_1)\r\n",
        "    ############################################\r\n",
        "\r\n",
        "    ################ PREDICTION ################\r\n",
        "    sig_pred        = torch.sigmoid(pred_step_2)\r\n",
        "    ############################################\r\n",
        "    return sig_pred\r\n",
        "\r\n",
        "    ################ SECONDARY NETWORK (FINE DECORDER) ENDS HERE ################\r\n",
        "\r\n",
        "  @staticmethod\r\n",
        "  def __batch_normalization(dim, name, **kwargs):\r\n",
        "    if   dim == 0 or dim == 1:  layer = nn.BatchNorm1d(**kwargs)\r\n",
        "    elif dim == 2:  layer = nn.BatchNorm2d(**kwargs)\r\n",
        "    elif dim == 3:  layer = nn.BatchNorm3d(**kwargs)\r\n",
        "    else:           raise NotImplementedError()\r\n",
        "    \"\"\"\r\n",
        "    if 'scale' in _weights_dict[name]:\r\n",
        "        layer.state_dict()['weight'].copy_(torch.from_numpy(_weights_dict[name]['scale']))\r\n",
        "    else:\r\n",
        "        layer.weight.data.fill_(1)\r\n",
        "\r\n",
        "    if 'bias' in _weights_dict[name]:\r\n",
        "        layer.state_dict()['bias'].copy_(torch.from_numpy(_weights_dict[name]['bias']))\r\n",
        "    else:\r\n",
        "        layer.bias.data.fill_(0)\r\n",
        "\r\n",
        "    layer.state_dict()['running_mean'].copy_(torch.from_numpy(_weights_dict[name]['mean']))\r\n",
        "    layer.state_dict()['running_var'].copy_(torch.from_numpy(_weights_dict[name]['var']))\r\n",
        "    \"\"\"\r\n",
        "    return layer\r\n",
        "\r\n",
        "  @staticmethod\r\n",
        "  def __conv(dim, name, **kwargs):\r\n",
        "    if   dim == 1:  layer = nn.Conv1d(**kwargs)\r\n",
        "    elif dim == 2:  layer = nn.Conv2d(**kwargs)\r\n",
        "    elif dim == 3:  layer = nn.Conv3d(**kwargs)\r\n",
        "    else:           raise NotImplementedError()\r\n",
        "\r\n",
        "    #layer.state_dict()['weight'].copy_(torch.from_numpy(_weights_dict[name]['weights']))\r\n",
        "    #if 'bias' in _weights_dict[name]:\r\n",
        "        #layer.state_dict()['bias'].copy_(torch.from_numpy(_weights_dict[name]['bias']))\r\n",
        "    return layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsAdxqKGa96m"
      },
      "source": [
        "# Initialise encoder object\r\n",
        "encoder = densenet121().double()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkdTieZAIigc"
      },
      "source": [
        "# Initialise decoder\r\n",
        "decoder = Decoder().double()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqq_jIhVIeGi"
      },
      "source": [
        "# Test Encoder\r\n",
        "img, target = sbd.__getitem__(4)\r\n",
        "pos_clicks, neg_clicks = gen_clicks(target)\r\n",
        "pos_map, neg_map = convert_clicks(pos_clicks, neg_clicks)\r\n",
        "input = torch.cat((img, pos_map, neg_map), 0)\r\n",
        "input = input.unsqueeze(0).double()\r\n",
        "encoder_out, dense_out = encoder(input)\r\n",
        "dense_out.append(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIQCkEUDIfLm"
      },
      "source": [
        "# Test Decoder\r\n",
        "decoder_out = decoder(dense_out)\r\n",
        "print(decoder_out.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qnqvDHfcshI"
      },
      "source": [
        "# Visualize Decoder output\r\n",
        "plt.imshow(transforms.ToPILImage()(decoder_out[0]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}